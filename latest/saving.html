<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Saving and restoring · TensorFlow.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-1123761-11', 'auto');
ga('send', 'pageview');
</script><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="index.html"><img class="logo" src="assets/logo.png" alt="TensorFlow.jl logo"/></a><h1>TensorFlow.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="tutorial.html">MNIST tutorial</a></li><li><a class="toctext" href="visualization.html">Visualizing with Tensorboard</a></li><li><a class="toctext" href="io.html">Using queues for loading your data</a></li><li><a class="toctext" href="shape_inference.html">Shape inference</a></li><li class="current"><a class="toctext" href="saving.html">Saving and restoring</a><ul class="internal"><li><a class="toctext" href="#Saving-and-restoring-variable-values-1">Saving and restoring variable values</a></li><li><a class="toctext" href="#Saving-and-restoring-models-1">Saving and restoring models</a></li></ul></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="core.html">Core functions</a></li><li><a class="toctext" href="ops.html">Operations</a></li><li><a class="toctext" href="io_ref.html">IO pipelines with queues</a></li><li><a class="toctext" href="summary_ref.html">Summaries</a></li></ul></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="basic_usage.html">Basic usage</a></li><li><a class="toctext" href="logistic.html">Logistic regression</a></li></ul></li><li><span class="toctext">Advanced</span><ul><li><a class="toctext" href="build_from_source.html">Build TensorFlow from source</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="saving.html">Saving and restoring</a></li></ul><a class="edit-page" href="https://github.com/malmaud/TensorFlow.jl/blob/master/docs/src/saving.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Saving and restoring</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Saving-and-restoring-network-graphs-and-variables-1" href="#Saving-and-restoring-network-graphs-and-variables-1">Saving and restoring network graphs and variables</a></h1><p>Both the value of variables (typically weights in a neural network) and the topology of the network (ie, all the operations that constitute your model) can be serialized to disk and loaded later. The mechanism is different for both.</p><h2><a class="nav-anchor" id="Saving-and-restoring-variable-values-1" href="#Saving-and-restoring-variable-values-1">Saving and restoring variable values</a></h2><p>See the <a href="https://www.tensorflow.org/programmers_guide/variables#saving_variables">main TensorFlow docs</a> for an overview.</p><p>First create a saver object:</p><pre><code class="language-julia">saver = train.Saver()</code></pre><p>Then call the <code>save</code> method to save your variables:</p><pre><code class="language-julia">train.save(saver, session, &quot;/path/to/variable_file&quot;)</code></pre><p>The newly-created &quot;variable_file&quot; is a <a href="https://github.com/simonster/JLD2.jl">JLD2</a> file that contains a mapping from variable names to their values. The value of variables can later be restored in a new Julia session with</p><pre><code class="language-julia">train.restore(saver, session, &quot;/path/to/variable_file&quot;)</code></pre><p>For example, in one Julia session I might have</p><pre><code class="language-julia">using TensorFlow
session = Session()
@tf x = get_variable([], Float32)
run(session, assign(x, 5.0f0))
saver = train.Saver()
train.save(saver, session, &quot;weights.jld&quot;)</code></pre><p>Then to restore in another session,</p><pre><code class="language-julia">using TensorFlow
session = Session()
@tf x = get_variable([], Float32)
saver = train.Saver()
train.restore(saver, session, &quot;weights.jld&quot;)
run(session, x)  # Outputs &#39;5.0f0&#39;</code></pre><p>Just as in the Python API, there is an easy way to save multiple variable files during different iterations of training, making it easy to retrospectively analyze the value of variables during training.</p><p><code>save</code> can be passed a <code>global_step</code> keyword parameter, which is an integer that will be suffixed to the variable file name. The <code>Saver</code> constructor accepts an optional <code>max_to_keep</code> argument, which is an integer specifying how many of the latest versions of the variable files to save (older ones will be discarded to save space). For example, this code will keep the value of variables during the 5 most recent training iterations:</p><pre><code class="language-julia"> ...
 saver = train.Saver(max_to_keep=5)
 for iteration in 1:100
   ...
   train.save(saver, session, &quot;variable_file&quot;, global_step=iteration)
end</code></pre><p>By the end of this loop, file &quot;variable_file_95&quot; contains the variable values during the 95th iteration, &quot;variable_file_96&quot; the 96th iteration, etc.</p><h2><a class="nav-anchor" id="Saving-and-restoring-models-1" href="#Saving-and-restoring-models-1">Saving and restoring models</a></h2><p>The actual structure of the model can also be saved and restored from disk. In TensorFlow jargon, the complete structure of the model is referred to as the &quot;metagraph&quot;.</p><p>To save the metagraph, call</p><pre><code class="language-julia">train.export_meta_graph(&quot;filename&quot;)</code></pre><p>To restore it, call</p><pre><code class="language-julia">train.import_meta_graph(&quot;filename&quot;)</code></pre><p>For example, in one Julia session you might write</p><pre><code class="language-julia">using TensorFlow
x = constant(1)
@tf y = x+1
train.export_meta_graph(&quot;my_model&quot;)</code></pre><p>Then in a new Julia session,</p><pre><code class="language-julia">using TensorFlow
session = Session()
train.import_meta_graph(&quot;my_model&quot;)
y = get_tensor_by_name(&quot;y&quot;)
run(session, y)  # Outputs &#39;2&#39;</code></pre><p>The metagraph file format is the same as that used by the Python TensorFlow version, so models can be freely passed to and from Python TensorFlow sessions.</p><footer><hr/><a class="previous" href="shape_inference.html"><span class="direction">Previous</span><span class="title">Shape inference</span></a><a class="next" href="core.html"><span class="direction">Next</span><span class="title">Core functions</span></a></footer></article></body></html>
