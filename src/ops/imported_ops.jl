# Autogenerated on 2017-09-21T18:03:58.585

module Ops
import TensorFlow
const tf = TensorFlow
"""
     equal(x, y)

Returns the truth value of (x == y) element-wise.

*NOTE*: `Equal` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function equal(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Equal")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Equal")
        tf.Tensor(tf.Operation(desc))
    end

"""
     not_equal(x, y)

Returns the truth value of (x != y) element-wise.

*NOTE*: `NotEqual` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function not_equal(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("NotEqual")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "NotEqual")
        tf.Tensor(tf.Operation(desc))
    end

"""
     less_equal(x, y)

Returns the truth value of (x <= y) element-wise.

*NOTE*: `LessEqual` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function less_equal(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LessEqual")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "LessEqual")
        tf.Tensor(tf.Operation(desc))
    end

"""
     greater(x, y)

Returns the truth value of (x > y) element-wise.

*NOTE*: `Greater` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function greater(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Greater")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Greater")
        tf.Tensor(tf.Operation(desc))
    end

"""
     greater_equal(x, y)

Returns the truth value of (x >= y) element-wise.

*NOTE*: `GreaterEqual` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function greater_equal(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("GreaterEqual")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "GreaterEqual")
        tf.Tensor(tf.Operation(desc))
    end

"""
     less(x, y)

Returns the truth value of (x < y) element-wise.

*NOTE*: `Less` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function less(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Less")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Less")
        tf.Tensor(tf.Operation(desc))
    end

"""
     no_op()

Does nothing. Only useful as a placeholder for control edges.


"""
tf.@op function no_op(; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("NoOp")
                end), name, "NoOp")
        tf.Tensor(tf.Operation(desc))
    end

"""
     count_up_to(ref)

Increments 'ref' until it reaches 'limit'.


"""
tf.@op function count_up_to(ref_; name=nothing, limit=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("CountUpTo")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    (ref_,) = tf.tf_promote(ref_)
                    tf.add_input(desc, ref_)
                    if limit !== nothing
                        desc["limit"] = Base.Int(limit)
                    end
                end), name, "CountUpTo")
        tf.Tensor(tf.Operation(desc))
    end

"""
     decode_gif(contents)

Decode the first frame of a GIF-encoded image to a uint8 tensor.

GIF with frame or transparency compression are not supported
convert animated GIF from compressed to uncompressed by:

    convert src.gif -coalesce dst.gif

This op also supports decoding JPEGs and PNGs, though it is cleaner to use
`tf.image.decode_image`.
"""
tf.@op function decode_gif(contents_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("DecodeGif")
                    contents_ = convert(TensorFlow.Tensor{String}, contents_)
                    tf.add_input(desc, contents_)
                end), name, "DecodeGif")
        tf.Tensor(tf.Operation(desc))
    end

"""
     decode_jpeg(contents; channels=0, ratio=1, fancy_upscaling=true, try_recover_truncated=false, acceptable_fraction=nothing, dct_method=)

Decode a JPEG-encoded image to a uint8 tensor.

The attr `channels` indicates the desired number of color channels for the
decoded image.

Accepted values are:

*   0: Use the number of channels in the JPEG-encoded image.
*   1: output a grayscale image.
*   3: output an RGB image.

If needed, the JPEG-encoded image is transformed to match the requested number
of color channels.

The attr `ratio` allows downscaling the image by an integer factor during
decoding.  Allowed values are: 1, 2, 4, and 8.  This is much faster than
downscaling the image later.

This op also supports decoding PNGs and non-animated GIFs since the interface is
the same, though it is cleaner to use `tf.image.decode_image`.
"""
tf.@op function decode_jpeg(contents_; name=nothing, channels=nothing, ratio=nothing, fancy_upscaling=nothing, try_recover_truncated=nothing, acceptable_fraction=nothing, dct_method=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("DecodeJpeg")
                    contents_ = convert(TensorFlow.Tensor{String}, contents_)
                    tf.add_input(desc, contents_)
                    if channels !== nothing
                        desc["channels"] = Base.Int(channels)
                    end
                    if ratio !== nothing
                        desc["ratio"] = Base.Int(ratio)
                    end
                    if fancy_upscaling !== nothing
                        desc["fancy_upscaling"] = Base.Bool(fancy_upscaling)
                    end
                    if try_recover_truncated !== nothing
                        desc["try_recover_truncated"] = Base.Bool(try_recover_truncated)
                    end
                    if acceptable_fraction !== nothing
                        desc["acceptable_fraction"] = Base.identity(acceptable_fraction)
                    end
                    if dct_method !== nothing
                        desc["dct_method"] = Base.String(dct_method)
                    end
                end), name, "DecodeJpeg")
        tf.Tensor(tf.Operation(desc))
    end

"""
     encode_jpeg(image; format=, quality=95, progressive=false, optimize_size=false, chroma_downsampling=true, density_unit=in, x_density=300, y_density=300, xmp_metadata=)

JPEG-encode an image.

`image` is a 3-D uint8 Tensor of shape `[height, width, channels]`.

The attr `format` can be used to override the color format of the encoded
output.  Values can be:

*   `''`: Use a default format based on the number of channels in the image.
*   `grayscale`: Output a grayscale JPEG image.  The `channels` dimension
    of `image` must be 1.
*   `rgb`: Output an RGB JPEG image. The `channels` dimension
    of `image` must be 3.

If `format` is not specified or is the empty string, a default format is picked
in function of the number of channels in `image`:

*   1: Output a grayscale image.
*   3: Output an RGB image.
"""
tf.@op function encode_jpeg(image_; name=nothing, format=nothing, quality=nothing, progressive=nothing, optimize_size=nothing, chroma_downsampling=nothing, density_unit=nothing, x_density=nothing, y_density=nothing, xmp_metadata=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("EncodeJpeg")
                    image_ = convert(TensorFlow.Tensor{UInt8}, image_)
                    tf.add_input(desc, image_)
                    if format !== nothing
                        desc["format"] = Base.String(format)
                    end
                    if quality !== nothing
                        desc["quality"] = Base.Int(quality)
                    end
                    if progressive !== nothing
                        desc["progressive"] = Base.Bool(progressive)
                    end
                    if optimize_size !== nothing
                        desc["optimize_size"] = Base.Bool(optimize_size)
                    end
                    if chroma_downsampling !== nothing
                        desc["chroma_downsampling"] = Base.Bool(chroma_downsampling)
                    end
                    if density_unit !== nothing
                        desc["density_unit"] = Base.String(density_unit)
                    end
                    if x_density !== nothing
                        desc["x_density"] = Base.Int(x_density)
                    end
                    if y_density !== nothing
                        desc["y_density"] = Base.Int(y_density)
                    end
                    if xmp_metadata !== nothing
                        desc["xmp_metadata"] = Base.String(xmp_metadata)
                    end
                end), name, "EncodeJpeg")
        tf.Tensor(tf.Operation(desc))
    end

"""
     encode_png(image; compression=-1)

PNG-encode an image.

`image` is a 3-D uint8 or uint16 Tensor of shape `[height, width, channels]`
where `channels` is:

*   1: for grayscale.
*   2: for grayscale + alpha.
*   3: for RGB.
*   4: for RGBA.

The ZLIB compression level, `compression`, can be -1 for the PNG-encoder
default or a value from 0 to 9.  9 is the highest compression level, generating
the smallest output, but is slower.
"""
tf.@op function encode_png(image_; name=nothing, compression=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("EncodePng")
                    image_ = convert(TensorFlow.Tensor{UInt8}, image_)
                    (image_,) = tf.tf_promote(image_)
                    tf.add_input(desc, image_)
                    if compression !== nothing
                        desc["compression"] = Base.Int(compression)
                    end
                end), name, "EncodePng")
        tf.Tensor(tf.Operation(desc))
    end

"""
     resize_area(images, size; align_corners=false)

Resize `images` to `size` using area interpolation.

Input images can be of different types but output images are always float.
"""
tf.@op function resize_area(images_, size_; name=nothing, align_corners=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ResizeArea")
                    images_ = convert(TensorFlow.Tensor{Any}, images_)
                    size_ = convert(TensorFlow.Tensor{Int32}, size_)
                    (images_,) = tf.tf_promote(images_)
                    tf.add_input(desc, images_)
                    tf.add_input(desc, size_)
                    if align_corners !== nothing
                        desc["align_corners"] = Base.Bool(align_corners)
                    end
                end), name, "ResizeArea")
        tf.Tensor(tf.Operation(desc))
    end

"""
     resize_bicubic(images, size; align_corners=false)

Resize `images` to `size` using bicubic interpolation.

Input images can be of different types but output images are always float.
"""
tf.@op function resize_bicubic(images_, size_; name=nothing, align_corners=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ResizeBicubic")
                    images_ = convert(TensorFlow.Tensor{Any}, images_)
                    size_ = convert(TensorFlow.Tensor{Int32}, size_)
                    (images_,) = tf.tf_promote(images_)
                    tf.add_input(desc, images_)
                    tf.add_input(desc, size_)
                    if align_corners !== nothing
                        desc["align_corners"] = Base.Bool(align_corners)
                    end
                end), name, "ResizeBicubic")
        tf.Tensor(tf.Operation(desc))
    end

"""
     resize_bilinear(images, size; align_corners=false)

Resize `images` to `size` using bilinear interpolation.

Input images can be of different types but output images are always float.
"""
tf.@op function resize_bilinear(images_, size_; name=nothing, align_corners=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ResizeBilinear")
                    images_ = convert(TensorFlow.Tensor{Any}, images_)
                    size_ = convert(TensorFlow.Tensor{Int32}, size_)
                    (images_,) = tf.tf_promote(images_)
                    tf.add_input(desc, images_)
                    tf.add_input(desc, size_)
                    if align_corners !== nothing
                        desc["align_corners"] = Base.Bool(align_corners)
                    end
                end), name, "ResizeBilinear")
        tf.Tensor(tf.Operation(desc))
    end

"""
     resize_nearest_neighbor(images, size; align_corners=false)

Resize `images` to `size` using nearest neighbor interpolation.


"""
tf.@op function resize_nearest_neighbor(images_, size_; name=nothing, align_corners=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ResizeNearestNeighbor")
                    images_ = convert(TensorFlow.Tensor{Any}, images_)
                    size_ = convert(TensorFlow.Tensor{Int32}, size_)
                    (images_,) = tf.tf_promote(images_)
                    tf.add_input(desc, images_)
                    tf.add_input(desc, size_)
                    if align_corners !== nothing
                        desc["align_corners"] = Base.Bool(align_corners)
                    end
                end), name, "ResizeNearestNeighbor")
        tf.Tensor(tf.Operation(desc))
    end

"""
     extract_glimpse(input, size, offsets; centered=true, normalized=true, uniform_noise=true)

Extracts a glimpse from the input tensor.

Returns a set of windows called glimpses extracted at location
`offsets` from the input tensor. If the windows only partially
overlaps the inputs, the non overlapping areas will be filled with
random noise.

The result is a 4-D tensor of shape `[batch_size, glimpse_height,
glimpse_width, channels]`. The channels and batch dimensions are the
same as that of the input tensor. The height and width of the output
windows are specified in the `size` parameter.

The argument `normalized` and `centered` controls how the windows are built:

* If the coordinates are normalized but not centered, 0.0 and 1.0
  correspond to the minimum and maximum of each height and width
  dimension.
* If the coordinates are both normalized and centered, they range from
  -1.0 to 1.0. The coordinates (-1.0, -1.0) correspond to the upper
  left corner, the lower right corner is located at (1.0, 1.0) and the
  center is at (0, 0).
* If the coordinates are not normalized they are interpreted as
  numbers of pixels.
"""
tf.@op function extract_glimpse(input_, size_, offsets_; name=nothing, centered=nothing, normalized=nothing, uniform_noise=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ExtractGlimpse")
                    input_ = convert(TensorFlow.Tensor{Float32}, input_)
                    size_ = convert(TensorFlow.Tensor{Int32}, size_)
                    offsets_ = convert(TensorFlow.Tensor{Float32}, offsets_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, size_)
                    tf.add_input(desc, offsets_)
                    if centered !== nothing
                        desc["centered"] = Base.Bool(centered)
                    end
                    if normalized !== nothing
                        desc["normalized"] = Base.Bool(normalized)
                    end
                    if uniform_noise !== nothing
                        desc["uniform_noise"] = Base.Bool(uniform_noise)
                    end
                end), name, "ExtractGlimpse")
        tf.Tensor(tf.Operation(desc))
    end

"""
     crop_and_resize(image, boxes, box_ind, crop_size; method=bilinear, extrapolation_value=nothing)

Extracts crops from the input image tensor and bilinearly resizes them (possibly

with aspect ratio change) to a common output size specified by `crop_size`. This
is more general than the `crop_to_bounding_box` op which extracts a fixed size
slice from the input image and does not allow resizing or aspect ratio change.

Returns a tensor with `crops` from the input `image` at positions defined at the
bounding box locations in `boxes`. The cropped boxes are all resized (with
bilinear interpolation) to a fixed `size = [crop_height, crop_width]`. The
result is a 4-D tensor `[num_boxes, crop_height, crop_width, depth]`.
"""
tf.@op function crop_and_resize(image_, boxes_, box_ind_, crop_size_; name=nothing, method=nothing, extrapolation_value=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("CropAndResize")
                    image_ = convert(TensorFlow.Tensor{Any}, image_)
                    boxes_ = convert(TensorFlow.Tensor{Float32}, boxes_)
                    box_ind_ = convert(TensorFlow.Tensor{Int32}, box_ind_)
                    crop_size_ = convert(TensorFlow.Tensor{Int32}, crop_size_)
                    (image_,) = tf.tf_promote(image_)
                    tf.add_input(desc, image_)
                    tf.add_input(desc, boxes_)
                    tf.add_input(desc, box_ind_)
                    tf.add_input(desc, crop_size_)
                    if method !== nothing
                        desc["method"] = Base.String(method)
                    end
                    if extrapolation_value !== nothing
                        desc["extrapolation_value"] = Base.identity(extrapolation_value)
                    end
                end), name, "CropAndResize")
        tf.Tensor(tf.Operation(desc))
    end

"""
     adjust_hue(images, delta)

Adjust the hue of one or more images.

`images` is a tensor of at least 3 dimensions.  The last dimension is
interpretted as channels, and must be three.

The input image is considered in the RGB colorspace. Conceptually, the RGB
colors are first mapped into HSV. A delta is then applied all the hue values,
and then remapped back to RGB colorspace.
"""
tf.@op function adjust_hue(images_, delta_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AdjustHue")
                    images_ = convert(TensorFlow.Tensor{Float32}, images_)
                    delta_ = convert(TensorFlow.Tensor{Float32}, delta_)
                    tf.add_input(desc, images_)
                    tf.add_input(desc, delta_)
                end), name, "AdjustHue")
        tf.Tensor(tf.Operation(desc))
    end

"""
     adjust_saturation(images, scale)

Adjust the saturation of one or more images.

`images` is a tensor of at least 3 dimensions.  The last dimension is
interpretted as channels, and must be three.

The input image is considered in the RGB colorspace. Conceptually, the RGB
colors are first mapped into HSV. A scale is then applied all the saturation
values, and then remapped back to RGB colorspace.
"""
tf.@op function adjust_saturation(images_, scale_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AdjustSaturation")
                    images_ = convert(TensorFlow.Tensor{Float32}, images_)
                    scale_ = convert(TensorFlow.Tensor{Float32}, scale_)
                    tf.add_input(desc, images_)
                    tf.add_input(desc, scale_)
                end), name, "AdjustSaturation")
        tf.Tensor(tf.Operation(desc))
    end

"""
     draw_bounding_boxes(images, boxes)

Draw bounding boxes on a batch of images.

Outputs a copy of `images` but draws on top of the pixels zero or more bounding
boxes specified by the locations in `boxes`. The coordinates of the each
bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example, if an image is 100 x 200 pixels and the bounding box is
`[0.1, 0.2, 0.5, 0.9]`, the bottom-left and upper-right coordinates of the
bounding box will be `(10, 40)` to `(50, 180)`.

Parts of the bounding box may fall outside the image.
"""
tf.@op function draw_bounding_boxes(images_, boxes_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("DrawBoundingBoxes")
                    images_ = convert(TensorFlow.Tensor{Float32}, images_)
                    boxes_ = convert(TensorFlow.Tensor{Float32}, boxes_)
                    (images_,) = tf.tf_promote(images_)
                    tf.add_input(desc, images_)
                    tf.add_input(desc, boxes_)
                end), name, "DrawBoundingBoxes")
        tf.Tensor(tf.Operation(desc))
    end

"""
     non_max_suppression(boxes, scores, max_output_size; iou_threshold=nothing)

Greedily selects a subset of bounding boxes in descending order of score,

pruning away boxes that have high intersection-over-union (IOU) overlap
with previously selected boxes.  Bounding boxes are supplied as
[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any
diagonal pair of box corners and the coordinates can be provided as normalized
(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm
is agnostic to where the origin is in the coordinate system.  Note that this
algorithm is invariant to orthogonal transformations and translations
of the coordinate system; thus translating or reflections of the coordinate
system result in the same boxes being selected by the algorithm.

The output of this operation is a set of integers indexing into the input
collection of bounding boxes representing the selected boxes.  The bounding
box coordinates corresponding to the selected indices can then be obtained
using the `tf.gather operation`.  For example:

  selected_indices = tf.image.non_max_suppression(
      boxes, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"""
tf.@op function non_max_suppression(boxes_, scores_, max_output_size_; name=nothing, iou_threshold=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("NonMaxSuppression")
                    boxes_ = convert(TensorFlow.Tensor{Float32}, boxes_)
                    scores_ = convert(TensorFlow.Tensor{Float32}, scores_)
                    max_output_size_ = convert(TensorFlow.Tensor{Int32}, max_output_size_)
                    tf.add_input(desc, boxes_)
                    tf.add_input(desc, scores_)
                    tf.add_input(desc, max_output_size_)
                    if iou_threshold !== nothing
                        desc["iou_threshold"] = Base.identity(iou_threshold)
                    end
                end), name, "NonMaxSuppression")
        tf.Tensor(tf.Operation(desc))
    end

"""
     sample_distorted_bounding_box(image_size, bounding_boxes; seed=0, seed2=0, min_object_covered=nothing, aspect_ratio_range=Int64[], area_range=Int64[], max_attempts=100, use_image_if_no_bounding_boxes=false)

Generate a single randomly distorted bounding box for an image.

Bounding box annotations are often supplied in addition to ground-truth labels
in image recognition or object localization tasks. A common technique for
training such a system is to randomly distort an image while preserving
its content, i.e. *data augmentation*. This Op outputs a randomly distorted
localization of an object, i.e. bounding box, given an `image_size`,
`bounding_boxes` and a series of constraints.

The output of this Op is a single bounding box that may be used to crop the
original image. The output is returned as 3 tensors: `begin`, `size` and
`bboxes`. The first 2 tensors can be fed directly into `tf.slice` to crop the
image. The latter may be supplied to `tf.image.draw_bounding_boxes` to visualize
what the bounding box looks like.

Bounding boxes are supplied and returned as `[y_min, x_min, y_max, x_max]`. The
bounding box coordinates are floats in `[0.0, 1.0]` relative to the width and
height of the underlying image.

For example,

```python
    # Generate a single distorted bounding box.
    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes)

    # Draw the bounding box in an image summary.
    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox_for_draw)
    tf.image_summary('images_with_box', image_with_box)

    # Employ the bounding box to distort the image.
    distorted_image = tf.slice(image, begin, size)
```

Note that if no bounding box information is available, setting
`use_image_if_no_bounding_boxes = true` will assume there is a single implicit
bounding box covering the whole image. If `use_image_if_no_bounding_boxes` is
false and no bounding boxes are supplied, an error is raised.
"""
tf.@op function sample_distorted_bounding_box(image_size_, bounding_boxes_; name=nothing, seed=nothing, seed2=nothing, min_object_covered=nothing, aspect_ratio_range=nothing, area_range=nothing, max_attempts=nothing, use_image_if_no_bounding_boxes=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SampleDistortedBoundingBox")
                    image_size_ = convert(TensorFlow.Tensor{Any}, image_size_)
                    bounding_boxes_ = convert(TensorFlow.Tensor{Float32}, bounding_boxes_)
                    (image_size_,) = tf.tf_promote(image_size_)
                    tf.add_input(desc, image_size_)
                    tf.add_input(desc, bounding_boxes_)
                    if seed !== nothing
                        desc["seed"] = Base.Int(seed)
                    end
                    if seed2 !== nothing
                        desc["seed2"] = Base.Int(seed2)
                    end
                    if min_object_covered !== nothing
                        desc["min_object_covered"] = Base.identity(min_object_covered)
                    end
                    if aspect_ratio_range !== nothing
                        desc["aspect_ratio_range"] = map(Base.identity, aspect_ratio_range)
                    end
                    if area_range !== nothing
                        desc["area_range"] = map(Base.identity, area_range)
                    end
                    if max_attempts !== nothing
                        desc["max_attempts"] = Base.Int(max_attempts)
                    end
                    if use_image_if_no_bounding_boxes !== nothing
                        desc["use_image_if_no_bounding_boxes"] = Base.Bool(use_image_if_no_bounding_boxes)
                    end
                end), name, "SampleDistortedBoundingBox")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:3
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     logical_and(x, y)

Returns the truth value of x AND y element-wise.

*NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function logical_and(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LogicalAnd")
                    x_ = convert(TensorFlow.Tensor{Bool}, x_)
                    y_ = convert(TensorFlow.Tensor{Bool}, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "LogicalAnd")
        tf.Tensor(tf.Operation(desc))
    end

"""
     logical_not(x)

Returns the truth value of NOT x element-wise.


"""
tf.@op function logical_not(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LogicalNot")
                    x_ = convert(TensorFlow.Tensor{Bool}, x_)
                    tf.add_input(desc, x_)
                end), name, "LogicalNot")
        tf.Tensor(tf.Operation(desc))
    end

"""
     logical_or(x, y)

Returns the truth value of x OR y element-wise.

*NOTE*: `LogicalOr` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function logical_or(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LogicalOr")
                    x_ = convert(TensorFlow.Tensor{Bool}, x_)
                    y_ = convert(TensorFlow.Tensor{Bool}, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "LogicalOr")
        tf.Tensor(tf.Operation(desc))
    end

"""
     add_n(inputs)

Add all input tensors element wise.


"""
tf.@op function add_n(inputs_; name=nothing, N=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AddN")
                    inputs_ = [convert(TensorFlow.Tensor{Any}, x) for x = inputs_]
                    (inputs_,) = tf.tf_promote(inputs_)
                    tf.add_input(desc, inputs_)
                    if N !== nothing
                        desc["N"] = Base.Int(N)
                    end
                end), name, "AddN")
        tf.Tensor(tf.Operation(desc))
    end

"""
     arg_min(input, dimension)

Returns the index with the smallest value across dimensions of a tensor.

Note that in case of ties the identity of the return value is not guaranteed.
"""
tf.@op function arg_min(input_, dimension_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ArgMin")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    dimension_ = convert(TensorFlow.Tensor{Int32}, dimension_)
                    dimension_ = dimension_ - convert(tf.Tensor{eltype(dimension_)}, 1)
                    (input_,) = tf.tf_promote(input_)
                    (dimension_,) = tf.tf_promote(dimension_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, dimension_)
                end), name, "ArgMin")
        tf.Tensor(tf.Operation(desc))
    end

"""
     arg_max(input, dimension)

Returns the index with the largest value across dimensions of a tensor.

Note that in case of ties the identity of the return value is not guaranteed.
"""
tf.@op function arg_max(input_, dimension_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ArgMax")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    dimension_ = convert(TensorFlow.Tensor{Int32}, dimension_)
                    dimension_ = dimension_ - convert(tf.Tensor{eltype(dimension_)}, 1)
                    (input_,) = tf.tf_promote(input_)
                    (dimension_,) = tf.tf_promote(dimension_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, dimension_)
                end), name, "ArgMax")
        tf.Tensor(tf.Operation(desc))
    end

"""
     add(x, y)

Returns x + y element-wise.

*NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function add(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Add")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Add")
        tf.Tensor(tf.Operation(desc))
    end

"""
     sub(x, y)

Returns x - y element-wise.

*NOTE*: `Sub` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function sub(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Sub")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Sub")
        tf.Tensor(tf.Operation(desc))
    end

"""
     mat_mul(a, b; transpose_a=false, transpose_b=false)

Multiply the matrix "a" by the matrix "b".

The inputs must be two-dimensional matrices and the inner dimension of
"a" (after being transposed if transpose_a is true) must match the
outer dimension of "b" (after being transposed if transposed_b is
true).

*Note*: The default kernel implementation for MatMul on GPUs uses
cublas.
"""
tf.@op function mat_mul(a_, b_; name=nothing, transpose_a=nothing, transpose_b=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MatMul")
                    a_ = convert(TensorFlow.Tensor{Any}, a_)
                    b_ = convert(TensorFlow.Tensor{Any}, b_)
                    (a_, b_) = tf.tf_promote(a_, b_)
                    tf.add_input(desc, a_)
                    tf.add_input(desc, b_)
                    if transpose_a !== nothing
                        desc["transpose_a"] = Base.Bool(transpose_a)
                    end
                    if transpose_b !== nothing
                        desc["transpose_b"] = Base.Bool(transpose_b)
                    end
                end), name, "MatMul")
        tf.Tensor(tf.Operation(desc))
    end

"""
     mul(x, y)

Returns x * y element-wise.

*NOTE*: `Mul` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function mul(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Mul")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Mul")
        tf.Tensor(tf.Operation(desc))
    end

"""
     pow(x, y)

Computes the power of one value to another.

Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for
corresponding elements in `x` and `y`. For example:

```
# tensor 'x' is [[2, 2]], [3, 3]]
# tensor 'y' is [[8, 16], [2, 3]]
tf.pow(x, y) ==> [[256, 65536], [9, 27]]
```
"""
tf.@op function pow(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Pow")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Pow")
        tf.Tensor(tf.Operation(desc))
    end

"""
     matrix_solve(matrix, rhs; adjoint=false)

Solves systems of linear equations.

`Matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions
form square matrices. `Rhs` is a tensor of shape `[..., M, K]`. The `output` is
a tensor shape `[..., M, K]`.  If `adjoint` is `False` then each output matrix
satisfies `matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.
If `adjoint` is `True` then each output matrix satisfies
`adjoint(matrix[..., :, :]) * output[..., :, :] = rhs[..., :, :]`.
"""
tf.@op function matrix_solve(matrix_, rhs_; name=nothing, adjoint=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MatrixSolve")
                    matrix_ = convert(TensorFlow.Tensor{Any}, matrix_)
                    rhs_ = convert(TensorFlow.Tensor{Any}, rhs_)
                    (matrix_, rhs_) = tf.tf_promote(matrix_, rhs_)
                    tf.add_input(desc, matrix_)
                    tf.add_input(desc, rhs_)
                    if adjoint !== nothing
                        desc["adjoint"] = Base.Bool(adjoint)
                    end
                end), name, "MatrixSolve")
        tf.Tensor(tf.Operation(desc))
    end

"""
     matrix_triangular_solve(matrix, rhs; lower=true, adjoint=false)

Solves systems of linear equations with upper or lower triangular matrices by

backsubstitution.

`matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions form
square matrices. If `lower` is `True` then the strictly upper triangular part
of each inner-most matrix is assumed to be zero and not accessed.
If `lower` is False then the strictly lower triangular part of each inner-most
matrix is assumed to be zero and not accessed.
`rhs` is a tensor of shape `[..., M, K]`.

The output is a tensor of shape `[..., M, K]`. If `adjoint` is
`True` then the innermost matrices in output` satisfy matrix equations
`matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.
If `adjoint` is `False` then the strictly then the  innermost matrices in
`output` satisfy matrix equations
`adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]`.
"""
tf.@op function matrix_triangular_solve(matrix_, rhs_; name=nothing, lower=nothing, adjoint=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MatrixTriangularSolve")
                    matrix_ = convert(TensorFlow.Tensor{Any}, matrix_)
                    rhs_ = convert(TensorFlow.Tensor{Any}, rhs_)
                    (matrix_, rhs_) = tf.tf_promote(matrix_, rhs_)
                    tf.add_input(desc, matrix_)
                    tf.add_input(desc, rhs_)
                    if lower !== nothing
                        desc["lower"] = Base.Bool(lower)
                    end
                    if adjoint !== nothing
                        desc["adjoint"] = Base.Bool(adjoint)
                    end
                end), name, "MatrixTriangularSolve")
        tf.Tensor(tf.Operation(desc))
    end

"""
     matrix_solve_ls(matrix, rhs, l2_regularizer; fast=true)

Solves one or more linear least-squares problems.

`matrix` is a tensor of shape `[..., M, N]` whose inner-most 2 dimensions
form matrices of size `[M, N]`. Rhs is a tensor of shape `[..., M, K]`.
The output is a tensor shape `[..., N, K]` where each output matrix solves
each of the equations matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]
in the least squares sense.

matrix and right-hand sides in the batch:

`matrix`=\\(A \in \Re^{m \times n}\\),
`rhs`=\\(B  \in \Re^{m \times k}\\),
`output`=\\(X  \in \Re^{n \times k}\\),
`l2_regularizer`=\\(\lambda\\).

If `fast` is `True`, then the solution is computed by solving the normal
equations using Cholesky decomposition. Specifically, if \\(m \ge n\\) then
\\(X = (A^T A + \lambda I)^{-1} A^T B\\), which solves the least-squares
problem \\(X = \mathrm{argmin}_{Z \in \Re^{n \times k} } ||A Z - B||_F^2 +
\lambda ||Z||_F^2\\). If \\(m \lt n\\) then `output` is computed as
\\(X = A^T (A A^T + \lambda I)^{-1} B\\), which (for \\(\lambda = 0\\)) is the
minimum-norm solution to the under-determined linear system, i.e.
\\(X = \mathrm{argmin}_{Z \in \Re^{n \times k} } ||Z||_F^2 \\), subject to
\\(A Z = B\\). Notice that the fast path is only numerically stable when
\\(A\\) is numerically full rank and has a condition number
\\(\mathrm{cond}(A) \lt \frac{1}{\sqrt{\epsilon_{mach} } }\\) or\\(\lambda\\) is
sufficiently large.

If `fast` is `False` an algorithm based on the numerically robust complete
orthogonal decomposition is used. This computes the minimum-norm
least-squares solution, even when \\(A\\) is rank deficient. This path is
typically 6-7 times slower than the fast path. If `fast` is `False` then
`l2_regularizer` is ignored.
"""
tf.@op function matrix_solve_ls(matrix_, rhs_, l2_regularizer_; name=nothing, fast=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MatrixSolveLs")
                    matrix_ = convert(TensorFlow.Tensor{Any}, matrix_)
                    rhs_ = convert(TensorFlow.Tensor{Any}, rhs_)
                    l2_regularizer_ = convert(TensorFlow.Tensor{Float64}, l2_regularizer_)
                    (matrix_, rhs_) = tf.tf_promote(matrix_, rhs_)
                    tf.add_input(desc, matrix_)
                    tf.add_input(desc, rhs_)
                    tf.add_input(desc, l2_regularizer_)
                    if fast !== nothing
                        desc["fast"] = Base.Bool(fast)
                    end
                end), name, "MatrixSolveLs")
        tf.Tensor(tf.Operation(desc))
    end

"""
     cholesky(input)

Computes the Cholesky decomposition of one or more square matrices.

The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions
form square matrices, with the same constraints as the single matrix Cholesky
decomposition above. The output is a tensor of the same shape as the input
containing the Cholesky decompositions for all input submatrices `[..., :, :]`.
"""
tf.@op function cholesky(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Cholesky")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "Cholesky")
        tf.Tensor(tf.Operation(desc))
    end

"""
     neg(x)

Computes numerical negative value element-wise.

I.e., \\(y = -x\\).
"""
tf.@op function neg(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Neg")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Neg")
        tf.Tensor(tf.Operation(desc))
    end

"""
     square(x)

Computes square of x element-wise.

I.e., \\(y = x * x = x^2\\).
"""
tf.@op function square(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Square")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Square")
        tf.Tensor(tf.Operation(desc))
    end

"""
     shape(input; out_type=Int32)

Returns the shape of a tensor.

This operation returns a 1-D integer tensor representing the shape of `input`.

For example:

```
# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]
shape(t) ==> [2, 2, 3]
```
"""
tf.@op function shape(input_; name=nothing, out_type=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Shape")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if out_type !== nothing
                        desc["out_type"] = Base.identity(out_type)
                    end
                end), name, "Shape")
        tf.Tensor(tf.Operation(desc))
    end

"""
     unsorted_segment_sum(data, segment_ids, num_segments)

Computes the sum along segments of a tensor.

Read @{math_ops#segmentationthe section on segmentation} for an explanation of
segments.

Computes a tensor such that
`(output[i] = sum_{j...} data[j...]` where the sum is over tuples `j...` such
that `segment_ids[j...] == i`.  Unlike `SegmentSum`, `segment_ids`
need not be sorted and need not cover all values in the full
range of valid values.

If the sum is empty for a given segment ID `i`, `output[i] = 0`.

`num_segments` should equal the number of distinct segment IDs.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/UnsortedSegmentSum.png" alt>
</div>
"""
tf.@op function unsorted_segment_sum(data_, segment_ids_, num_segments_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("UnsortedSegmentSum")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    segment_ids_ = convert(TensorFlow.Tensor{Any}, segment_ids_)
                    segment_ids_ = segment_ids_ - convert(tf.Tensor{eltype(segment_ids_)}, 1)
                    num_segments_ = convert(TensorFlow.Tensor{Int32}, num_segments_)
                    (data_,) = tf.tf_promote(data_)
                    (segment_ids_,) = tf.tf_promote(segment_ids_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, segment_ids_)
                    tf.add_input(desc, num_segments_)
                end), name, "UnsortedSegmentSum")
        tf.Tensor(tf.Operation(desc))
    end

"""
     unsorted_segment_max(data, segment_ids, num_segments)

Computes the Max along segments of a tensor.

Read @{math_ops#segmentationthe section on segmentation} for an explanation of
segments.

This operator is similar to the [unsorted segment sum operator](../../../api_docs/python/math_ops.md#UnsortedSegmentSum).
Instead of computing the sum over segments, it computes the maximum
such that:

\\(output_i = \max_j data_j\\) where max is over `j` such
that `segment_ids[j] == i`.

If the maximum is empty for a given segment ID `i`, it outputs the smallest possible value for specific numeric type,
 `output[i] = numeric_limits<T>::min()`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/UnsortedSegmentSum.png" alt>
</div>
"""
tf.@op function unsorted_segment_max(data_, segment_ids_, num_segments_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("UnsortedSegmentMax")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    segment_ids_ = convert(TensorFlow.Tensor{Any}, segment_ids_)
                    segment_ids_ = segment_ids_ - convert(tf.Tensor{eltype(segment_ids_)}, 1)
                    num_segments_ = convert(TensorFlow.Tensor{Int32}, num_segments_)
                    (data_,) = tf.tf_promote(data_)
                    (segment_ids_,) = tf.tf_promote(segment_ids_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, segment_ids_)
                    tf.add_input(desc, num_segments_)
                end), name, "UnsortedSegmentMax")
        tf.Tensor(tf.Operation(desc))
    end

"""
     segment_sum(data, segment_ids)

Computes the sum along segments of a tensor.

Read @{math_ops#segmentationthe section on segmentation} for an explanation of
segments.

Computes a tensor such that
\\(output_i = \sum_j data_j\\) where sum is over `j` such
that `segment_ids[j] == i`.

If the sum is empty for a given segment ID `i`, `output[i] = 0`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentSum.png" alt>
</div>
"""
tf.@op function segment_sum(data_, segment_ids_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SegmentSum")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    segment_ids_ = convert(TensorFlow.Tensor{Any}, segment_ids_)
                    segment_ids_ = segment_ids_ - convert(tf.Tensor{eltype(segment_ids_)}, 1)
                    (data_,) = tf.tf_promote(data_)
                    (segment_ids_,) = tf.tf_promote(segment_ids_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, segment_ids_)
                end), name, "SegmentSum")
        tf.Tensor(tf.Operation(desc))
    end

"""
     segment_max(data, segment_ids)

Computes the maximum along segments of a tensor.

Read @{math_ops#segmentationthe section on segmentation} for an explanation of
segments.

Computes a tensor such that
\\(output_i = \max_j(data_j)\\) where `max` is over `j` such
that `segment_ids[j] == i`.

If the max is empty for a given segment ID `i`, `output[i] = 0`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentMax.png" alt>
</div>
"""
tf.@op function segment_max(data_, segment_ids_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SegmentMax")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    segment_ids_ = convert(TensorFlow.Tensor{Any}, segment_ids_)
                    segment_ids_ = segment_ids_ - convert(tf.Tensor{eltype(segment_ids_)}, 1)
                    (data_,) = tf.tf_promote(data_)
                    (segment_ids_,) = tf.tf_promote(segment_ids_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, segment_ids_)
                end), name, "SegmentMax")
        tf.Tensor(tf.Operation(desc))
    end

"""
     segment_mean(data, segment_ids)

Computes the mean along segments of a tensor.

Read @{math_ops#segmentationthe section on segmentation} for an explanation of
segments.

Computes a tensor such that
\\(output_i = \frac{\sum_j data_j}{N}\\) where `mean` is
over `j` such that `segment_ids[j] == i` and `N` is the total number of
values summed.

If the mean is empty for a given segment ID `i`, `output[i] = 0`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentMean.png" alt>
</div>
"""
tf.@op function segment_mean(data_, segment_ids_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SegmentMean")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    segment_ids_ = convert(TensorFlow.Tensor{Any}, segment_ids_)
                    segment_ids_ = segment_ids_ - convert(tf.Tensor{eltype(segment_ids_)}, 1)
                    (data_,) = tf.tf_promote(data_)
                    (segment_ids_,) = tf.tf_promote(segment_ids_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, segment_ids_)
                end), name, "SegmentMean")
        tf.Tensor(tf.Operation(desc))
    end

"""
     segment_min(data, segment_ids)

Computes the minimum along segments of a tensor.

Read @{math_ops#segmentationthe section on segmentation} for an explanation of
segments.

Computes a tensor such that
\\(output_i = \min_j(data_j)\\) where `min` is over `j` such
that `segment_ids[j] == i`.

If the min is empty for a given segment ID `i`, `output[i] = 0`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentMin.png" alt>
</div>
"""
tf.@op function segment_min(data_, segment_ids_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SegmentMin")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    segment_ids_ = convert(TensorFlow.Tensor{Any}, segment_ids_)
                    segment_ids_ = segment_ids_ - convert(tf.Tensor{eltype(segment_ids_)}, 1)
                    (data_,) = tf.tf_promote(data_)
                    (segment_ids_,) = tf.tf_promote(segment_ids_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, segment_ids_)
                end), name, "SegmentMin")
        tf.Tensor(tf.Operation(desc))
    end

"""
     segment_prod(data, segment_ids)

Computes the product along segments of a tensor.

Read @{math_ops#segmentationthe section on segmentation} for an explanation of
segments.

Computes a tensor such that
\\(output_i = \prod_j data_j\\) where the product is over `j` such
that `segment_ids[j] == i`.

If the product is empty for a given segment ID `i`, `output[i] = 1`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/SegmentProd.png" alt>
</div>
"""
tf.@op function segment_prod(data_, segment_ids_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SegmentProd")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    segment_ids_ = convert(TensorFlow.Tensor{Any}, segment_ids_)
                    segment_ids_ = segment_ids_ - convert(tf.Tensor{eltype(segment_ids_)}, 1)
                    (data_,) = tf.tf_promote(data_)
                    (segment_ids_,) = tf.tf_promote(segment_ids_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, segment_ids_)
                end), name, "SegmentProd")
        tf.Tensor(tf.Operation(desc))
    end

"""
     relu(features)

Computes rectified linear: `max(features, 0)`.


"""
tf.@op function relu(features_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Relu")
                    features_ = convert(TensorFlow.Tensor{Any}, features_)
                    (features_,) = tf.tf_promote(features_)
                    tf.add_input(desc, features_)
                end), name, "Relu")
        tf.Tensor(tf.Operation(desc))
    end

"""
     relu6(features)

Computes rectified linear 6: `min(max(features, 0), 6)`.


"""
tf.@op function relu6(features_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Relu6")
                    features_ = convert(TensorFlow.Tensor{Any}, features_)
                    (features_,) = tf.tf_promote(features_)
                    tf.add_input(desc, features_)
                end), name, "Relu6")
        tf.Tensor(tf.Operation(desc))
    end

"""
     elu(features)

Computes exponential linear: `exp(features) - 1` if < 0, `features` otherwise.

See [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
](http://arxiv.org/abs/1511.07289)
"""
tf.@op function elu(features_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Elu")
                    features_ = convert(TensorFlow.Tensor{Any}, features_)
                    (features_,) = tf.tf_promote(features_)
                    tf.add_input(desc, features_)
                end), name, "Elu")
        tf.Tensor(tf.Operation(desc))
    end

"""
     softplus(features)

Computes softplus: `log(exp(features) + 1)`.


"""
tf.@op function softplus(features_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Softplus")
                    features_ = convert(TensorFlow.Tensor{Any}, features_)
                    (features_,) = tf.tf_promote(features_)
                    tf.add_input(desc, features_)
                end), name, "Softplus")
        tf.Tensor(tf.Operation(desc))
    end

"""
     softsign(features)

Computes softsign: `features / (abs(features) + 1)`.


"""
tf.@op function softsign(features_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Softsign")
                    features_ = convert(TensorFlow.Tensor{Any}, features_)
                    (features_,) = tf.tf_promote(features_)
                    tf.add_input(desc, features_)
                end), name, "Softsign")
        tf.Tensor(tf.Operation(desc))
    end

"""
     softmax(logits)

Computes softmax activations.

For each batch `i` and class `j` we have

    softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))
"""
tf.@op function softmax(logits_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Softmax")
                    logits_ = convert(TensorFlow.Tensor{Any}, logits_)
                    (logits_,) = tf.tf_promote(logits_)
                    tf.add_input(desc, logits_)
                end), name, "Softmax")
        tf.Tensor(tf.Operation(desc))
    end

"""
     sigmoid(x)

Computes sigmoid of `x` element-wise.

Specifically, `y = 1 / (1 + exp(-x))`.
"""
tf.@op function sigmoid(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Sigmoid")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Sigmoid")
        tf.Tensor(tf.Operation(desc))
    end

"""
     conv3d(input, filter; data_format=NDHWC)

Computes a 3-D convolution given 5-D `input` and `filter` tensors.

In signal processing, cross-correlation is a measure of similarity of
two waveforms as a function of a time-lag applied to one of them. This
is also known as a sliding dot product or sliding inner-product.

Our Conv3D implements a form of cross-correlation.
"""
tf.@op function conv3d(input_, filter_; name=nothing, strides=nothing, padding=nothing, data_format=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Conv3D")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    filter_ = convert(TensorFlow.Tensor{Any}, filter_)
                    (input_, filter_) = tf.tf_promote(input_, filter_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, filter_)
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                    if data_format !== nothing
                        desc["data_format"] = Base.String(data_format)
                    end
                end), name, "Conv3D")
        tf.Tensor(tf.Operation(desc))
    end

"""
     max_pool(input; data_format=NHWC)

Performs max pooling on the input.


"""
tf.@op function max_pool(input_; name=nothing, ksize=nothing, strides=nothing, padding=nothing, data_format=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MaxPool")
                    input_ = convert(TensorFlow.Tensor{Float32}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if ksize !== nothing
                        desc["ksize"] = map(Base.identity, ksize)
                    end
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                    if data_format !== nothing
                        desc["data_format"] = Base.String(data_format)
                    end
                end), name, "MaxPool")
        tf.Tensor(tf.Operation(desc))
    end

"""
     max_pool3d(input; data_format=NDHWC)

Performs 3D max pooling on the input.


"""
tf.@op function max_pool3d(input_; name=nothing, ksize=nothing, strides=nothing, padding=nothing, data_format=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MaxPool3D")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if ksize !== nothing
                        desc["ksize"] = map(Base.identity, ksize)
                    end
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                    if data_format !== nothing
                        desc["data_format"] = Base.String(data_format)
                    end
                end), name, "MaxPool3D")
        tf.Tensor(tf.Operation(desc))
    end

"""
     avg_pool(value; data_format=NHWC)

Performs average pooling on the input.

Each entry in `output` is the mean of the corresponding size `ksize`
window in `value`.
"""
tf.@op function avg_pool(value_; name=nothing, ksize=nothing, strides=nothing, padding=nothing, data_format=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AvgPool")
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (value_,) = tf.tf_promote(value_)
                    tf.add_input(desc, value_)
                    if ksize !== nothing
                        desc["ksize"] = map(Base.identity, ksize)
                    end
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                    if data_format !== nothing
                        desc["data_format"] = Base.String(data_format)
                    end
                end), name, "AvgPool")
        tf.Tensor(tf.Operation(desc))
    end

"""
     avg_pool3d(input; data_format=NDHWC)

Performs 3D average pooling on the input.


"""
tf.@op function avg_pool3d(input_; name=nothing, ksize=nothing, strides=nothing, padding=nothing, data_format=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AvgPool3D")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if ksize !== nothing
                        desc["ksize"] = map(Base.identity, ksize)
                    end
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                    if data_format !== nothing
                        desc["data_format"] = Base.String(data_format)
                    end
                end), name, "AvgPool3D")
        tf.Tensor(tf.Operation(desc))
    end

"""
     log_softmax(logits)

Computes log softmax activations.

For each batch `i` and class `j` we have

    logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))
"""
tf.@op function log_softmax(logits_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LogSoftmax")
                    logits_ = convert(TensorFlow.Tensor{Any}, logits_)
                    (logits_,) = tf.tf_promote(logits_)
                    tf.add_input(desc, logits_)
                end), name, "LogSoftmax")
        tf.Tensor(tf.Operation(desc))
    end

"""
     dilation2d(input, filter)

Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.

The `input` tensor has shape `[batch, in_height, in_width, depth]` and the
`filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each
input channel is processed independently of the others with its own structuring
function. The `output` tensor has shape
`[batch, out_height, out_width, depth]`. The spatial dimensions of the output
tensor depend on the `padding` algorithm. We currently only support the default
"NHWC" `data_format`.

In detail, the grayscale morphological 2-D dilation is the max-sum correlation
(for consistency with `conv2d`, we use unmirrored filters):

    output[b, y, x, c] =
       max_{dy, dx} input[b,
                          strides[1] * y + rates[1] * dy,
                          strides[2] * x + rates[2] * dx,
                          c] +
                    filter[dy, dx, c]

Max-pooling is a special case when the filter has size equal to the pooling
kernel size and contains all zeros.

Note on duality: The dilation of `input` by the `filter` is equal to the
negation of the erosion of `-input` by the reflected `filter`.
"""
tf.@op function dilation2d(input_, filter_; name=nothing, strides=nothing, rates=nothing, padding=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Dilation2D")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    filter_ = convert(TensorFlow.Tensor{Any}, filter_)
                    (input_, filter_) = tf.tf_promote(input_, filter_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, filter_)
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if rates !== nothing
                        desc["rates"] = map(Base.identity, rates)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                end), name, "Dilation2D")
        tf.Tensor(tf.Operation(desc))
    end

"""
     conv2d(input, filter; use_cudnn_on_gpu=true, data_format=NHWC)

Computes a 2-D convolution given 4-D `input` and `filter` tensors.

Given an input tensor of shape `[batch, in_height, in_width, in_channels]`
and a filter / kernel tensor of shape
`[filter_height, filter_width, in_channels, out_channels]`, this op
performs the following:

1. Flattens the filter to a 2-D matrix with shape
   `[filter_height * filter_width * in_channels, output_channels]`.
2. Extracts image patches from the input tensor to form a *virtual*
   tensor of shape `[batch, out_height, out_width,
   filter_height * filter_width * in_channels]`.
3. For each patch, right-multiplies the filter matrix and the image patch
   vector.

In detail, with the default NHWC format,

    output[b, i, j, k] =
        sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *
                        filter[di, dj, q, k]

Must have `strides[0] = strides[3] = 1`.  For the most common case of the same
horizontal and vertices strides, `strides = [1, stride, stride, 1]`.
"""
tf.@op function conv2d(input_, filter_; name=nothing, strides=nothing, use_cudnn_on_gpu=nothing, padding=nothing, data_format=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Conv2D")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    filter_ = convert(TensorFlow.Tensor{Any}, filter_)
                    (input_, filter_) = tf.tf_promote(input_, filter_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, filter_)
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if use_cudnn_on_gpu !== nothing
                        desc["use_cudnn_on_gpu"] = Base.Bool(use_cudnn_on_gpu)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                    if data_format !== nothing
                        desc["data_format"] = Base.String(data_format)
                    end
                end), name, "Conv2D")
        tf.Tensor(tf.Operation(desc))
    end

"""
     random_uniform(shape; seed=0, seed2=0)

Outputs random values from a uniform distribution.

The generated values follow a uniform distribution in the range `[0, 1)`. The
lower bound 0 is included in the range, while the upper bound 1 is excluded.
"""
tf.@op function random_uniform(shape_; name=nothing, seed=nothing, seed2=nothing, dtype=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("RandomUniform")
                    shape_ = convert(TensorFlow.Tensor{Any}, shape_)
                    (shape_,) = tf.tf_promote(shape_)
                    tf.add_input(desc, shape_)
                    if seed !== nothing
                        desc["seed"] = Base.Int(seed)
                    end
                    if seed2 !== nothing
                        desc["seed2"] = Base.Int(seed2)
                    end
                    if dtype !== nothing
                        desc["dtype"] = Base.identity(dtype)
                    end
                end), name, "RandomUniform")
        tf.Tensor(tf.Operation(desc))
    end

"""
     random_standard_normal(shape; seed=0, seed2=0)

Outputs random values from a normal distribution.

The generated values will have mean 0 and standard deviation 1.
"""
tf.@op function random_standard_normal(shape_; name=nothing, seed=nothing, seed2=nothing, dtype=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("RandomStandardNormal")
                    shape_ = convert(TensorFlow.Tensor{Any}, shape_)
                    (shape_,) = tf.tf_promote(shape_)
                    tf.add_input(desc, shape_)
                    if seed !== nothing
                        desc["seed"] = Base.Int(seed)
                    end
                    if seed2 !== nothing
                        desc["seed2"] = Base.Int(seed2)
                    end
                    if dtype !== nothing
                        desc["dtype"] = Base.identity(dtype)
                    end
                end), name, "RandomStandardNormal")
        tf.Tensor(tf.Operation(desc))
    end

"""
     random_shuffle(value; seed=0, seed2=0)

Randomly shuffles a tensor along its first dimension.

  The tensor is shuffled along dimension 0, such that each `value[j]` is mapped
  to one and only one `output[i]`. For example, a mapping that might occur for a
  3x2 tensor is:

```prettyprint
[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
```
"""
tf.@op function random_shuffle(value_; name=nothing, seed=nothing, seed2=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("RandomShuffle")
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (value_,) = tf.tf_promote(value_)
                    tf.add_input(desc, value_)
                    if seed !== nothing
                        desc["seed"] = Base.Int(seed)
                    end
                    if seed2 !== nothing
                        desc["seed2"] = Base.Int(seed2)
                    end
                end), name, "RandomShuffle")
        tf.Tensor(tf.Operation(desc))
    end

"""
     strided_slice(input, begin, end, strides; begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0)

Return a strided slice from `input`.

Note, most python users will want to use the Python `Tensor.__getitem__`
or `Variable.__getitem__` rather than this op directly.

The goal of this op is to produce a new tensor with a subset of
the elements from the `n` dimensional `input` tensor. The subset is chosen using
a sequence of `m` sparse range specifications encoded into the arguments
of this function. Note, in some cases
`m` could be equal to `n`, but this need not be the case. Each
range specification entry can be one of the following:

- An ellipsis (...). Ellipses are used to imply zero or more
  dimensions of full-dimension selection and are produced using
  `ellipsis_mask`. For example, `foo[...]` is the identity slice.

- A new axis. This is used to insert a new shape=1 dimension and is
  produced using `new_axis_mask`. For example, `foo[:, ...]` where
  `foo` is shape `(3, 4)` produces a `(1, 3, 4)` tensor.


- A range `begin:end:stride`. This is used to specify how much to choose from
  a given dimension. `stride` can be any integer but 0.  `begin` is an integer
  which represents the index of the first value to select while `end` represents
  the index of the last value to select. The number of values selected in each
  dimension is `end - begin` if `stride > 0` and `begin - end` if `stride < 0`.
  `begin` and `end` can be negative where `-1` is the last element, `-2` is
  the second to last. `begin_mask` controls whether to replace the explicitly
  given `begin` with an implicit effective value of `0` if `stride > 0` and
  `-1` if `stride < 0`. `end_mask` is analogous but produces the number
  required to create the largest open interval. For example, given a shape
  `(3,)` tensor `foo[:]`, the effective `begin` and `end` are `0` and `3`. Do
  not assume this is equivalent to `foo[0:-1]` which has an effective `begin`
  and `end` of `0` and `2`. Another example is `foo[-2::-1]` which reverses the
  first dimension of a tensor while dropping the last two (in the original
  order elements). For example `foo = [1,2,3,4]; foo[-2::-1]` is `[4,3]`.

- A single index. This is used to keep only elements that have a given
  index. For example (`foo[2, :]` on a shape `(5,6)` tensor produces a
  shape `(6,)` tensor. This is encoded in `begin` and `end` and
  `shrink_axis_mask`.

Each conceptual range specification is encoded in the op's argument. This
encoding is best understand by considering a non-trivial example. In
particular,
`foo[1, 2:4, None, ..., :-3:-1, :]` will be encoded as

```
begin = [1, 2, x, x, 0, x] # x denotes don't care (usually 0)
end = [2, 4, x, x, -3, x]
strides = [1, 1, x, x, -1, 1]
begin_mask = 1<<4 | 1 << 5 = 48
end_mask = 1<<5 = 32
ellipsis_mask = 1<<3 = 8
new_axis_mask = 1<<2 4
shrink_axis_mask = 1<<0
```

In this case if `foo.shape` is (5, 5, 5, 5, 5, 5) the final shape of
the slice becomes (2, 1, 5, 5, 2, 5).
Let us walk step by step through each argument specification.

1.  The first argument in the example slice is turned into `begin = 1` and
`end = begin + 1 = 2`. To disambiguate from the original spec `2:4` we
also set the appropriate bit in `shrink_axis_mask`.

2. `2:4` is contributes 2, 4, 1 to begin, end, and stride. All masks have
zero bits contributed.

3. None is a synonym for `tf.newaxis`. This means insert a dimension of size 1
dimension in the final shape. Dummy values are contributed to begin,
end and stride, while the new_axis_mask bit is set.

4. `...` grab the full ranges from as many dimensions as needed to
fully specify a slice for every dimension of the input shape.

5. `:-3:-1` shows the use of negative indices. A negative index `i` associated
with a dimension that has shape `s` is converted to a positive index
`s + i`. So `-1` becomes `s-1` (i.e. the last element). This conversion
is done internally so begin, end and strides receive x, -3, and -1.
The appropriate begin_mask bit is set to indicate the start range is the
full range (ignoring the x).

6. `:` indicates that the entire contents of the corresponding dimension
is selected. This is equivalent to `::` or `0::1`. begin, end, and strides
receive 0, 0, and 1, respectively. The appropriate bits in `begin_mask` and
`end_mask` are also set.

*Requirements*:
  `0 != strides[i] for i in [0, m)`
  `ellipsis_mask must be a power of two (only one ellipsis)`
"""
tf.@op function strided_slice(input_, begin_, end_, strides_; name=nothing, Index=nothing, begin_mask=nothing, end_mask=nothing, ellipsis_mask=nothing, new_axis_mask=nothing, shrink_axis_mask=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("StridedSlice")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    begin_ = convert(TensorFlow.Tensor{Any}, begin_)
                    begin_ = begin_ - convert(tf.Tensor{eltype(begin_)}, 1)
                    end_ = convert(TensorFlow.Tensor{Any}, end_)
                    end_ = end_ - convert(tf.Tensor{eltype(end_)}, 1)
                    strides_ = convert(TensorFlow.Tensor{Any}, strides_)
                    strides_ = strides_ - convert(tf.Tensor{eltype(strides_)}, 1)
                    (input_,) = tf.tf_promote(input_)
                    (begin_, end_, strides_) = tf.tf_promote(begin_, end_, strides_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, begin_)
                    tf.add_input(desc, end_)
                    tf.add_input(desc, strides_)
                    if Index !== nothing
                        desc["Index"] = Base.identity(Index)
                    end
                    if begin_mask !== nothing
                        begin_mask = Base.Int(begin_mask) - 1
                    end
                    if begin_mask !== nothing
                        desc["begin_mask"] = Base.Int(begin_mask)
                    end
                    if end_mask !== nothing
                        end_mask = Base.Int(end_mask) - 1
                    end
                    if end_mask !== nothing
                        desc["end_mask"] = Base.Int(end_mask)
                    end
                    if ellipsis_mask !== nothing
                        ellipsis_mask = Base.Int(ellipsis_mask) - 1
                    end
                    if ellipsis_mask !== nothing
                        desc["ellipsis_mask"] = Base.Int(ellipsis_mask)
                    end
                    if new_axis_mask !== nothing
                        new_axis_mask = Base.Int(new_axis_mask) - 1
                    end
                    if new_axis_mask !== nothing
                        desc["new_axis_mask"] = Base.Int(new_axis_mask)
                    end
                    if shrink_axis_mask !== nothing
                        shrink_axis_mask = Base.Int(shrink_axis_mask) - 1
                    end
                    if shrink_axis_mask !== nothing
                        desc["shrink_axis_mask"] = Base.Int(shrink_axis_mask)
                    end
                end), name, "StridedSlice")
        tf.Tensor(tf.Operation(desc))
    end

"""
     expand_dims(input, dim)

Inserts a dimension of 1 into a tensor's shape.

Given a tensor `input`, this operation inserts a dimension of 1 at the
dimension index `dim` of `input`'s shape. The dimension index `dim` starts at
zero; if you specify a negative number for `dim` it is counted backward from
the end.

This operation is useful if you want to add a batch dimension to a single
element. For example, if you have a single image of shape `[height, width,
channels]`, you can make it a batch of 1 image with `expand_dims(image, 0)`,
which will make the shape `[1, height, width, channels]`.

Other examples:

```
# 't' is a tensor of shape [2]
shape(expand_dims(t, 0)) ==> [1, 2]
shape(expand_dims(t, 1)) ==> [2, 1]
shape(expand_dims(t, -1)) ==> [2, 1]

# 't2' is a tensor of shape [2, 3, 5]
shape(expand_dims(t2, 0)) ==> [1, 2, 3, 5]
shape(expand_dims(t2, 2)) ==> [2, 3, 1, 5]
shape(expand_dims(t2, 3)) ==> [2, 3, 5, 1]
```

This operation requires that:

`-1-input.dims() <= dim <= input.dims()`

This operation is related to `squeeze()`, which removes dimensions of
size 1.
"""
tf.@op function expand_dims(input_, dim_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ExpandDims")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    dim_ = convert(TensorFlow.Tensor{Int32}, dim_)
                    dim_ = dim_ - convert(tf.Tensor{eltype(dim_)}, 1)
                    (input_,) = tf.tf_promote(input_)
                    (dim_,) = tf.tf_promote(dim_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, dim_)
                end), name, "ExpandDims")
        tf.Tensor(tf.Operation(desc))
    end

"""
     tile(input, multiples)

Constructs a tensor by tiling a given tensor.

This operation creates a new tensor by replicating `input` `multiples` times.
The output tensor's i'th dimension has `input.dims(i) * multiples[i]` elements,
and the values of `input` are replicated `multiples[i]` times along the 'i'th
dimension. For example, tiling `[a b c d]` by `[2]` produces
`[a b c d a b c d]`.
"""
tf.@op function tile(input_, multiples_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Tile")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    multiples_ = convert(TensorFlow.Tensor{Int32}, multiples_)
                    (input_,) = tf.tf_promote(input_)
                    (multiples_,) = tf.tf_promote(multiples_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, multiples_)
                end), name, "Tile")
        tf.Tensor(tf.Operation(desc))
    end

"""
     pad(input, paddings)

Pads a tensor with zeros.

This operation pads a `input` with zeros according to the `paddings` you
specify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is the
rank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates
how many zeros to add before the contents of `input` in that dimension, and
`paddings[D, 1]` indicates how many zeros to add after the contents of `input`
in that dimension.

The padded size of each dimension D of the output is:

`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`

For example:

```
# 't' is [[1, 1], [2, 2]]
# 'paddings' is [[1, 1], [2, 2]]
# rank of 't' is 2
pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]
                      [0, 0, 1, 1, 0, 0]
                      [0, 0, 2, 2, 0, 0]
                      [0, 0, 0, 0, 0, 0]]
```
"""
tf.@op function pad(input_, paddings_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Pad")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    paddings_ = convert(TensorFlow.Tensor{Int32}, paddings_)
                    (input_,) = tf.tf_promote(input_)
                    (paddings_,) = tf.tf_promote(paddings_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, paddings_)
                end), name, "Pad")
        tf.Tensor(tf.Operation(desc))
    end

"""
     gather(params, indices; validate_indices=true)

Gather slices from `params` according to `indices`.

`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).
Produces an output tensor with shape `indices.shape + params.shape[1:]` where:

```python
    # Scalar indices
    output[:, ..., :] = params[indices, :, ... :]

    # Vector indices
    output[i, :, ..., :] = params[indices[i], :, ... :]

    # Higher rank indices
    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]
```

If `indices` is a permutation and `len(indices) == params.shape[0]` then
this operation will permute `params` accordingly.

`validate_indices`: DEPRECATED. If this operation is assigned to CPU, values in
`indices` are always validated to be within range. If assigned to GPU,
out-of-bound indices result in safe but unspecified behavior, which may include
raising an error.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/Gather.png" alt>
</div>
"""
tf.@op function gather(params_, indices_; name=nothing, validate_indices=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Gather")
                    params_ = convert(TensorFlow.Tensor{Any}, params_)
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    (params_,) = tf.tf_promote(params_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, params_)
                    tf.add_input(desc, indices_)
                    if validate_indices !== nothing
                        desc["validate_indices"] = Base.Bool(validate_indices)
                    end
                end), name, "Gather")
        tf.Tensor(tf.Operation(desc))
    end

"""
     gather_nd(params, indices)

Gather values or slices from `params` according to `indices`.

`indices` is an integer tensor containing indices into `params`.  The last
dimension of `indices` can be at most the rank of `params`:

    indices.shape[-1] <= params.rank

The last dimension of `indices` corresponds to elements
(if `indices.shape[-1] = params.rank`) or slices
(if `indices.shape[-1] < params.rank`) along dimension `indices.shape[-1]`
of `params`.  The output tensor has shape

    indices.shape[:-1] + params.shape[indices.shape[-1]:]

Some examples below.

Simple indexing into a matrix:

```python
    indices = [[0, 0], [1, 1]]
    params = [['a', 'b'], ['c', 'd']]
    output = ['a', 'd']
```

Slice indexing into a matrix:

```python
    indices = [[1], [0]]
    params = [['a', 'b'], ['c', 'd']]
    output = [['c', 'd'], ['a', 'b']]
```

Indexing into a 3-tensor:

```python
    indices = [[1]]
    params = [[['a0', 'b0'], ['c0', 'd0']],
              [['a1', 'b1'], ['c1', 'd1']]]
    output = [[['a1', 'b1'], ['c1', 'd1']]]


    indices = [[0, 1], [1, 0]]
    params = [[['a0', 'b0'], ['c0', 'd0']],
              [['a1', 'b1'], ['c1', 'd1']]]
    output = [['c0', 'd0'], ['a1', 'b1']]


    indices = [[0, 0, 1], [1, 0, 1]]
    params = [[['a0', 'b0'], ['c0', 'd0']],
              [['a1', 'b1'], ['c1', 'd1']]]
    output = ['b0', 'b1']
```

Batched indexing into a matrix:

```python
    indices = [[[0, 0]], [[0, 1]]]
    params = [['a', 'b'], ['c', 'd']]
    output = [['a'], ['b']]
```

Batched slice indexing into a matrix:

```python
    indices = [[[1]], [[0]]]
    params = [['a', 'b'], ['c', 'd']]
    output = [[['c', 'd']], [['a', 'b']]]
```

Batched indexing into a 3-tensor:

```python
    indices = [[[1]], [[0]]]
    params = [[['a0', 'b0'], ['c0', 'd0']],
              [['a1', 'b1'], ['c1', 'd1']]]
    output = [[[['a1', 'b1'], ['c1', 'd1']]],
              [[['a0', 'b0'], ['c0', 'd0']]]]

    indices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]]
    params = [[['a0', 'b0'], ['c0', 'd0']],
              [['a1', 'b1'], ['c1', 'd1']]]
    output = [[['c0', 'd0'], ['a1', 'b1']],
              [['a0', 'b0'], ['c1', 'd1']]]


    indices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]]
    params = [[['a0', 'b0'], ['c0', 'd0']],
              [['a1', 'b1'], ['c1', 'd1']]]
    output = [['b0', 'b1'], ['d0', 'c1']]
```
"""
tf.@op function gather_nd(params_, indices_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("GatherNd")
                    params_ = convert(TensorFlow.Tensor{Any}, params_)
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    (params_,) = tf.tf_promote(params_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, params_)
                    tf.add_input(desc, indices_)
                end), name, "GatherNd")
        tf.Tensor(tf.Operation(desc))
    end

"""
     scatter_nd(indices, updates, shape)

Scatter `updates` into a new (initially zero) tensor according to `indices`.

Creates a new tensor by applying sparse `updates` to individual
values or slices within a zero tensor of the given `shape` according to
indices.  This operator is the inverse of the [tf.gather_nd](#gather_nd)
operator which extracts values or slices from a given tensor.

**WARNING**: The order in which updates are applied is nondeterministic, so the
output will be nondeterministic if `indices` contains duplicates.

`indices` is an integer tensor containing indices into a new tensor of shape
`shape`.  The last dimension of `indices` can be at most the rank of `shape`:

    indices.shape[-1] <= shape.rank

The last dimension of `indices` corresponds to indices into elements
(if `indices.shape[-1] = shape.rank`) or slices
(if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of
`shape`.  `updates` is a tensor with shape

    indices.shape[:-1] + shape[indices.shape[-1]:]

The simplest form of scatter is to insert individual elements in a tensor by
index. For example, say we want to insert 4 scattered elements in a rank-1
tensor with 8 elements.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterNd1.png" alt>
</div>

In Python, this scatter operation would look like this:

```python
    indices = tf.constant([[4], [3], [1], [7]])
    updates = tf.constant([9, 10, 11, 12])
    shape = tf.constant([8])
    scatter = tf.scatter_nd(indices, updates, shape)
    with tf.Session() as sess:
      print(sess.run(scatter))
```

The resulting tensor would look like this:

    [0, 11, 0, 10, 9, 0, 0, 12]

We can also, insert entire slices of a higher rank tensor all at once. For
example, if we wanted to insert two slices in the first dimension of a
rank-3 tensor with two matrices of new values.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterNd2.png" alt>
</div>

In Python, this scatter operation would look like this:

```python
    indices = tf.constant([[0], [2]])
    updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],
                            [7, 7, 7, 7], [8, 8, 8, 8]],
                           [[5, 5, 5, 5], [6, 6, 6, 6],
                            [7, 7, 7, 7], [8, 8, 8, 8]]])
    shape = tf.constant([4, 4, 4])
    scatter = tf.scatter_nd(indices, updates, shape)
    with tf.Session() as sess:
      print(sess.run(scatter))
```

The resulting tensor would look like this:

    [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
     [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
     [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
     [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]
"""
tf.@op function scatter_nd(indices_, updates_, shape_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ScatterNd")
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    updates_ = convert(TensorFlow.Tensor{Any}, updates_)
                    shape_ = convert(TensorFlow.Tensor{Any}, shape_)
                    (updates_,) = tf.tf_promote(updates_)
                    (indices_, shape_) = tf.tf_promote(indices_, shape_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, updates_)
                    tf.add_input(desc, shape_)
                end), name, "ScatterNd")
        tf.Tensor(tf.Operation(desc))
    end

"""
     dynamic_partition(data, partitions)

Partitions `data` into `num_partitions` tensors using indices from `partitions`.

For each index tuple `js` of size `partitions.ndim`, the slice `data[js, ...]`
becomes part of `outputs[partitions[js]]`.  The slices with `partitions[js] = i`
are placed in `outputs[i]` in lexicographic order of `js`, and the first
dimension of `outputs[i]` is the number of entries in `partitions` equal to `i`.
In detail,

```python
    outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]

    outputs[i] = pack([data[js, ...] for js if partitions[js] == i])
```

`data.shape` must start with `partitions.shape`.

For example:

```python
    # Scalar partitions.
    partitions = 1
    num_partitions = 2
    data = [10, 20]
    outputs[0] = []  # Empty with shape [0, 2]
    outputs[1] = [[10, 20]]

    # Vector partitions.
    partitions = [0, 0, 1, 1, 0]
    num_partitions = 2
    data = [10, 20, 30, 40, 50]
    outputs[0] = [10, 20, 50]
    outputs[1] = [30, 40]
```

See `dynamic_stitch` for an example on how to merge partitions back.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/DynamicPartition.png" alt>
</div>
"""
tf.@op function dynamic_partition(data_, partitions_; name=nothing, num_partitions=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("DynamicPartition")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    partitions_ = convert(TensorFlow.Tensor{Int32}, partitions_)
                    (data_,) = tf.tf_promote(data_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, partitions_)
                    if num_partitions !== nothing
                        desc["num_partitions"] = Base.Int(num_partitions)
                    end
                end), name, "DynamicPartition")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:num_partitions
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     dynamic_stitch(indices, data)

Interleave the values from the `data` tensors into a single tensor.

Builds a merged tensor such that

```python
    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]
```

For example, if each `indices[m]` is scalar or vector, we have

```python
    # Scalar indices:
    merged[indices[m], ...] = data[m][...]

    # Vector indices:
    merged[indices[m][i], ...] = data[m][i, ...]
```

Each `data[i].shape` must start with the corresponding `indices[i].shape`,
and the rest of `data[i].shape` must be constant w.r.t. `i`.  That is, we
must have `data[i].shape = indices[i].shape + constant`.  In terms of this
`constant`, the output shape is

    merged.shape = [max(indices)] + constant

Values are merged in order, so if an index appears in both `indices[m][i]` and
`indices[n][j]` for `(m,i) < (n,j)` the slice `data[n][j]` will appear in the
merged result.

For example:

```python
    indices[0] = 6
    indices[1] = [4, 1]
    indices[2] = [[5, 2], [0, 3]]
    data[0] = [61, 62]
    data[1] = [[41, 42], [11, 12]]
    data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]
    merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],
              [51, 52], [61, 62]]
```

This method can be used to merge partitions created by `dynamic_partition`
as illustrated on the following example:

```python
    # Apply function (increments x_i) on elements for which a certain condition
    # apply (x_i != -1 in this example).
    x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])
    condition_mask=tf.not_equal(x,tf.constant(-1.))
    partitioned_data = tf.dynamic_partition(
        x, tf.cast(condition_mask, tf.int32) , 2)
    partitioned_data[1] = partitioned_data[1] + 1.0
    condition_indices = tf.dynamic_partition(
        tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)
    x = tf.dynamic_stitch(condition_indices, partitioned_data)
    # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain
    # unchanged.
```

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/DynamicStitch.png" alt>
</div>
"""
tf.@op function dynamic_stitch(indices_, data_; name=nothing, N=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("DynamicStitch")
                    indices_ = [convert(TensorFlow.Tensor{Int32}, x) for x = indices_]
                    data_ = [convert(TensorFlow.Tensor{Any}, x) for x = data_]
                    (data_,) = tf.tf_promote(data_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, data_)
                    if N !== nothing
                        desc["N"] = Base.Int(N)
                    end
                end), name, "DynamicStitch")
        tf.Tensor(tf.Operation(desc))
    end

"""
     pack(values; axis=0)

Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor.

Packs the `N` tensors in `values` into a tensor with rank one higher than each
tensor in `values`, by packing them along the `axis` dimension.
Given a list of tensors of shape `(A, B, C)`;

if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.
if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.
Etc.

For example:

```
# 'x' is [1, 4]
# 'y' is [2, 5]
# 'z' is [3, 6]
pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.
pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]
```

This is the opposite of `unpack`.
"""
tf.@op function pack(values_; name=nothing, N=nothing, axis=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Pack")
                    values_ = [convert(TensorFlow.Tensor{Any}, x) for x = values_]
                    (values_,) = tf.tf_promote(values_)
                    tf.add_input(desc, values_)
                    if N !== nothing
                        desc["N"] = Base.Int(N)
                    end
                    if axis !== nothing
                        axis = Base.Int(axis) - 1
                    end
                    if axis !== nothing
                        desc["axis"] = Base.Int(axis)
                    end
                end), name, "Pack")
        tf.Tensor(tf.Operation(desc))
    end

"""
     concat_v2(values, axis)

Concatenates tensors along one dimension.


"""
tf.@op function concat_v2(values_, axis_; name=nothing, N=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ConcatV2")
                    values_ = [convert(TensorFlow.Tensor{Any}, x) for x = values_]
                    axis_ = convert(TensorFlow.Tensor{Int32}, axis_)
                    axis_ = axis_ - convert(tf.Tensor{eltype(axis_)}, 1)
                    (values_,) = tf.tf_promote(values_)
                    (axis_,) = tf.tf_promote(axis_)
                    tf.add_input(desc, values_)
                    tf.add_input(desc, axis_)
                    if N !== nothing
                        desc["N"] = Base.Int(N)
                    end
                end), name, "ConcatV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     self_adjoint_eig_v2(input; compute_v=true)

Computes the eigen decomposition of one or more square self-adjoint matrices.

Computes the eigenvalues and (optionally) eigenvectors of each inner matrix in
`input` such that `input[..., :, :] = v[..., :, :] * diag(e[..., :])`.

```prettyprint
# a is a tensor.
# e is a tensor of eigenvalues.
# v is a tensor of eigenvectors.
e, v = self_adjoint_eig(a)
e = self_adjoint_eig(a, compute_v=False)
```
"""
tf.@op function self_adjoint_eig_v2(input_; name=nothing, compute_v=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SelfAdjointEigV2")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if compute_v !== nothing
                        desc["compute_v"] = Base.Bool(compute_v)
                    end
                end), name, "SelfAdjointEigV2")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:2
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     is_finite(x)

Returns which elements of x are finite.

@compatibility(numpy)
Equivalent to np.isfinite
@end_compatibility
"""
tf.@op function is_finite(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("IsFinite")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "IsFinite")
        tf.Tensor(tf.Operation(desc))
    end

"""
     is_nan(x)

Returns which elements of x are NaN.

@compatibility(numpy)
Equivalent to np.isnan
@end_compatibility
"""
tf.@op function is_nan(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("IsNan")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "IsNan")
        tf.Tensor(tf.Operation(desc))
    end

"""
     is_inf(x)

Returns which elements of x are Inf.

@compatibility(numpy)
Equivalent to np.isinf
@end_compatibility
"""
tf.@op function is_inf(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("IsInf")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "IsInf")
        tf.Tensor(tf.Operation(desc))
    end

"""
     lrn(input; depth_radius=5, bias=nothing, alpha=nothing, beta=nothing)

Local Response Normalization.

The 4-D `input` tensor is treated as a 3-D array of 1-D vectors (along the last
dimension), and each vector is normalized independently.  Within a given vector,
each component is divided by the weighted, squared sum of inputs within
`depth_radius`.  In detail,

    sqr_sum[a, b, c, d] =
        sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
    output = input / (bias + alpha * sqr_sum) ** beta

For details, see [Krizhevsky et al., ImageNet classification with deep
convolutional neural networks (NIPS 2012)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).
"""
tf.@op function lrn(input_; name=nothing, depth_radius=nothing, bias=nothing, alpha=nothing, beta=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LRN")
                    input_ = convert(TensorFlow.Tensor{Float32}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if depth_radius !== nothing
                        desc["depth_radius"] = Base.Int(depth_radius)
                    end
                    if bias !== nothing
                        desc["bias"] = Base.identity(bias)
                    end
                    if alpha !== nothing
                        desc["alpha"] = Base.identity(alpha)
                    end
                    if beta !== nothing
                        desc["beta"] = Base.identity(beta)
                    end
                end), name, "LRN")
        tf.Tensor(tf.Operation(desc))
    end

"""
     assign(ref, value; validate_shape=true, use_locking=true)

Update 'ref' by assigning 'value' to it.

This operation outputs "ref" after the assignment is done.
This makes it easier to chain operations that need to use the reset value.
"""
tf.@op function assign(ref_, value_; name=nothing, validate_shape=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Assign")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (ref_, value_) = tf.tf_promote(ref_, value_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, value_)
                    if validate_shape !== nothing
                        desc["validate_shape"] = Base.Bool(validate_shape)
                    end
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "Assign")
        tf.Tensor(tf.Operation(desc))
    end

"""
     assign_add(ref, value; use_locking=false)

Update 'ref' by adding 'value' to it.

This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.
"""
tf.@op function assign_add(ref_, value_; name=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AssignAdd")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (ref_, value_) = tf.tf_promote(ref_, value_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, value_)
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "AssignAdd")
        tf.Tensor(tf.Operation(desc))
    end

"""
     assign_sub(ref, value; use_locking=false)

Update 'ref' by subtracting 'value' from it.

This operation outputs "ref" after the update is done.
This makes it easier to chain operations that need to use the reset value.
"""
tf.@op function assign_sub(ref_, value_; name=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AssignSub")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (ref_, value_) = tf.tf_promote(ref_, value_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, value_)
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "AssignSub")
        tf.Tensor(tf.Operation(desc))
    end

"""
     scatter_update(ref, indices, updates; use_locking=true)

Applies sparse updates to a variable reference.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] = updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] = updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] = updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

If values in `ref` is to be updated more than once, because there are
duplicate entries in `indices`, the order at which the updates happen
for each value is undefined.

Requires `updates.shape = indices.shape + ref.shape[1:]`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterUpdate.png" alt>
</div>
"""
tf.@op function scatter_update(ref_, indices_, updates_; name=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ScatterUpdate")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    updates_ = convert(TensorFlow.Tensor{Any}, updates_)
                    (ref_, updates_) = tf.tf_promote(ref_, updates_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, updates_)
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "ScatterUpdate")
        tf.Tensor(tf.Operation(desc))
    end

"""
     scatter_sub(ref, indices, updates; use_locking=false)

Subtracts sparse updates to a variable reference.

```python
    # Scalar indices
    ref[indices, ...] -= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] -= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] -= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their (negated) contributions add.

Requires `updates.shape = indices.shape + ref.shape[1:]`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterSub.png" alt>
</div>
"""
tf.@op function scatter_sub(ref_, indices_, updates_; name=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ScatterSub")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    updates_ = convert(TensorFlow.Tensor{Any}, updates_)
                    (ref_, updates_) = tf.tf_promote(ref_, updates_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, updates_)
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "ScatterSub")
        tf.Tensor(tf.Operation(desc))
    end

"""
     scatter_add(ref, indices, updates; use_locking=false)

Adds sparse updates to a variable reference.

This operation computes

    # Scalar indices
    ref[indices, ...] += updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] += updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] += updates[i, ..., j, ...]

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions add.

Requires `updates.shape = indices.shape + ref.shape[1:]`.

<div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="https://www.tensorflow.org/images/ScatterAdd.png" alt>
</div>
"""
tf.@op function scatter_add(ref_, indices_, updates_; name=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ScatterAdd")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    updates_ = convert(TensorFlow.Tensor{Any}, updates_)
                    (ref_, updates_) = tf.tf_promote(ref_, updates_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, updates_)
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "ScatterAdd")
        tf.Tensor(tf.Operation(desc))
    end

"""
     scatter_mul(ref, indices, updates; use_locking=false)

Multiplies sparse updates into a variable reference.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] *= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] *= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions multiply.

Requires `updates.shape = indices.shape + ref.shape[1:]`.
"""
tf.@op function scatter_mul(ref_, indices_, updates_; name=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ScatterMul")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    updates_ = convert(TensorFlow.Tensor{Any}, updates_)
                    (ref_, updates_) = tf.tf_promote(ref_, updates_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, updates_)
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "ScatterMul")
        tf.Tensor(tf.Operation(desc))
    end

"""
     scatter_div(ref, indices, updates; use_locking=false)

Divides a variable reference by sparse updates.

This operation computes

```python
    # Scalar indices
    ref[indices, ...] /= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] /= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
```

This operation outputs `ref` after the update is done.
This makes it easier to chain operations that need to use the reset value.

Duplicate entries are handled correctly: if multiple `indices` reference
the same location, their contributions divide.

Requires `updates.shape = indices.shape + ref.shape[1:]`.
"""
tf.@op function scatter_div(ref_, indices_, updates_; name=nothing, use_locking=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ScatterDiv")
                    ref_ = convert(TensorFlow.Tensor{Any}, ref_)
                    indices_ = convert(TensorFlow.Tensor{Any}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    updates_ = convert(TensorFlow.Tensor{Any}, updates_)
                    (ref_, updates_) = tf.tf_promote(ref_, updates_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, ref_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, updates_)
                    if use_locking !== nothing
                        desc["use_locking"] = Base.Bool(use_locking)
                    end
                end), name, "ScatterDiv")
        tf.Tensor(tf.Operation(desc))
    end

"""
     merge_summary(inputs)

Merges summaries.

This op creates a
[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)
protocol buffer that contains the union of all the values in the input
summaries.

When the Op is run, it reports an `InvalidArgument` error if multiple values
in the summaries to merge use the same tag.
"""
tf.@op function merge_summary(inputs_; name=nothing, N=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MergeSummary")
                    inputs_ = [convert(TensorFlow.Tensor{String}, x) for x = inputs_]
                    tf.add_input(desc, inputs_)
                    if N !== nothing
                        desc["N"] = Base.Int(N)
                    end
                end), name, "MergeSummary")
        tf.Tensor(tf.Operation(desc))
    end

"""
     scalar_summary(tags, values)

Outputs a `Summary` protocol buffer with scalar values.

The input `tags` and `values` must have the same shape.  The generated summary
has a summary value for each tag-value pair in `tags` and `values`.
"""
tf.@op function scalar_summary(tags_, values_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ScalarSummary")
                    tags_ = convert(TensorFlow.Tensor{String}, tags_)
                    values_ = convert(TensorFlow.Tensor{Any}, values_)
                    (values_,) = tf.tf_promote(values_)
                    tf.add_input(desc, tags_)
                    tf.add_input(desc, values_)
                end), name, "ScalarSummary")
        tf.Tensor(tf.Operation(desc))
    end

"""
     audio_summary_v2(tag, tensor, sample_rate; max_outputs=3)

Outputs a `Summary` protocol buffer with audio.

The summary has up to `max_outputs` summary values containing audio. The
audio is built from `tensor` which must be 3-D with shape `[batch_size,
frames, channels]` or 2-D with shape `[batch_size, frames]`. The values are
assumed to be in the range of `[-1.0, 1.0]` with a sample rate of `sample_rate`.

The `tag` argument is a scalar `Tensor` of type `string`.  It is used to
build the `tag` of the summary values:

*  If `max_outputs` is 1, the summary value tag is '*tag*/audio'.
*  If `max_outputs` is greater than 1, the summary value tags are
   generated sequentially as '*tag*/audio/0', '*tag*/audio/1', etc.
"""
tf.@op function audio_summary_v2(tag_, tensor_, sample_rate_; name=nothing, max_outputs=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("AudioSummaryV2")
                    tag_ = convert(TensorFlow.Tensor{String}, tag_)
                    tensor_ = convert(TensorFlow.Tensor{Float32}, tensor_)
                    sample_rate_ = convert(TensorFlow.Tensor{Float32}, sample_rate_)
                    tf.add_input(desc, tag_)
                    tf.add_input(desc, tensor_)
                    tf.add_input(desc, sample_rate_)
                    if max_outputs !== nothing
                        desc["max_outputs"] = Base.Int(max_outputs)
                    end
                end), name, "AudioSummaryV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     histogram_summary(tag, values)

Outputs a `Summary` protocol buffer with a histogram.

The generated
[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)
has one summary value containing a histogram for `values`.

This op reports an `InvalidArgument` error if any value is not finite.
"""
tf.@op function histogram_summary(tag_, values_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("HistogramSummary")
                    tag_ = convert(TensorFlow.Tensor{String}, tag_)
                    values_ = convert(TensorFlow.Tensor{Float32}, values_)
                    (values_,) = tf.tf_promote(values_)
                    tf.add_input(desc, tag_)
                    tf.add_input(desc, values_)
                end), name, "HistogramSummary")
        tf.Tensor(tf.Operation(desc))
    end

"""
     image_summary(tag, tensor; max_images=3, bad_color=?)

Outputs a `Summary` protocol buffer with images.

The summary has up to `max_images` summary values containing images. The
images are built from `tensor` which must be 4-D with shape `[batch_size,
height, width, channels]` and where `channels` can be:

*  1: `tensor` is interpreted as Grayscale.
*  3: `tensor` is interpreted as RGB.
*  4: `tensor` is interpreted as RGBA.

The images have the same number of channels as the input tensor. For float
input, the values are normalized one image at a time to fit in the range
`[0, 255]`.  `uint8` values are unchanged.  The op uses two different
normalization algorithms:

*  If the input values are all positive, they are rescaled so the largest one
   is 255.

*  If any input value is negative, the values are shifted so input value 0.0
   is at 127.  They are then rescaled so that either the smallest value is 0,
   or the largest one is 255.

The `tag` argument is a scalar `Tensor` of type `string`.  It is used to
build the `tag` of the summary values:

*  If `max_images` is 1, the summary value tag is '*tag*/image'.
*  If `max_images` is greater than 1, the summary value tags are
   generated sequentially as '*tag*/image/0', '*tag*/image/1', etc.

The `bad_color` argument is the color to use in the generated images for
non-finite input values.  It is a `unit8` 1-D tensor of length `channels`.
Each element must be in the range `[0, 255]` (It represents the value of a
pixel in the output image).  Non-finite values in the input tensor are
replaced by this tensor in the output image.  The default value is the color
red.
"""
tf.@op function image_summary(tag_, tensor_; name=nothing, max_images=nothing, bad_color=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ImageSummary")
                    tag_ = convert(TensorFlow.Tensor{String}, tag_)
                    tensor_ = convert(TensorFlow.Tensor{Float32}, tensor_)
                    (tensor_,) = tf.tf_promote(tensor_)
                    tf.add_input(desc, tag_)
                    tf.add_input(desc, tensor_)
                    if max_images !== nothing
                        desc["max_images"] = Base.Int(max_images)
                    end
                    if bad_color !== nothing
                        desc["bad_color"] = TensorFlow.RawTensor(bad_color)
                    end
                end), name, "ImageSummary")
        tf.Tensor(tf.Operation(desc))
    end

"""
     decode_png(contents; channels=0, dtype=UInt8)

Decode a PNG-encoded image to a uint8 or uint16 tensor.

The attr `channels` indicates the desired number of color channels for the
decoded image.

Accepted values are:

*   0: Use the number of channels in the PNG-encoded image.
*   1: output a grayscale image.
*   3: output an RGB image.
*   4: output an RGBA image.

If needed, the PNG-encoded image is transformed to match the requested number
of color channels.

This op also supports decoding JPEGs and non-animated GIFs since the interface
is the same, though it is cleaner to use `tf.image.decode_image`.
"""
tf.@op function decode_png(contents_; name=nothing, channels=nothing, dtype=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("DecodePng")
                    contents_ = convert(TensorFlow.Tensor{String}, contents_)
                    tf.add_input(desc, contents_)
                    if channels !== nothing
                        desc["channels"] = Base.Int(channels)
                    end
                    if dtype !== nothing
                        desc["dtype"] = Base.identity(dtype)
                    end
                end), name, "DecodePng")
        tf.Tensor(tf.Operation(desc))
    end

"""
     where(input)

Returns locations of true values in a boolean tensor.

This operation returns the coordinates of true elements in `input`. The
coordinates are returned in a 2-D tensor where the first dimension (rows)
represents the number of true elements, and the second dimension (columns)
represents the coordinates of the true elements. Keep in mind, the shape of
the output tensor can vary depending on how many true values there are in
`input`. Indices are output in row-major order.

For example:

```
# 'input' tensor is [[True, False]
#                    [True, False]]
# 'input' has two true values, so output has two coordinates.
# 'input' has rank of 2, so coordinates have two indices.
where(input) ==> [[0, 0],
                  [1, 0]]

# `input` tensor is [[[True, False]
#                     [True, False]]
#                    [[False, True]
#                     [False, True]]
#                    [[False, False]
#                     [False, True]]]
# 'input' has 5 true values, so output has 5 coordinates.
# 'input' has rank of 3, so coordinates have three indices.
where(input) ==> [[0, 0, 0],
                  [0, 1, 0],
                  [1, 0, 1],
                  [1, 1, 1],
                  [2, 1, 1]]
```
"""
tf.@op function where(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Where")
                    input_ = convert(TensorFlow.Tensor{Bool}, input_)
                    tf.add_input(desc, input_)
                end), name, "Where")
        tf.Tensor(tf.Operation(desc))
    end

"""
     const_()

Returns a constant tensor.


"""
tf.@op function const_(; name=nothing, value=nothing, dtype=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Const")
                    if value !== nothing
                        desc["value"] = TensorFlow.RawTensor(value)
                    end
                    if dtype !== nothing
                        desc["dtype"] = Base.identity(dtype)
                    end
                end), name, "Const")
        tf.Tensor(tf.Operation(desc))
    end

"""
     variable_v2(; container=, shared_name=)

Holds state in the form of a tensor that persists across steps.

Outputs a ref to the tensor state so it may be read or modified.
TODO(zhifengc/mrry): Adds a pointer to a more detail document
about sharing states in tensorflow.
"""
tf.@op function variable_v2(; name=nothing, shape=nothing, dtype=nothing, container=nothing, shared_name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("VariableV2")
                    if shape !== nothing
                        desc["shape"] = Base.identity(shape)
                    end
                    if dtype !== nothing
                        desc["dtype"] = Base.identity(dtype)
                    end
                    if container !== nothing
                        desc["container"] = Base.String(container)
                    end
                    if shared_name !== nothing
                        desc["shared_name"] = Base.String(shared_name)
                    end
                end), name, "VariableV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     log(x)

Computes natural logarithm of x element-wise.

I.e., \\(y = \log_e x\\).
"""
tf.@op function log(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Log")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Log")
        tf.Tensor(tf.Operation(desc))
    end

"""
     exp(x)

Computes exponential of x element-wise.  \\(y = e^x\\).


"""
tf.@op function exp(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Exp")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Exp")
        tf.Tensor(tf.Operation(desc))
    end

"""
     ceil(x)

Returns element-wise smallest integer in not less than x.


"""
tf.@op function ceil(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Ceil")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Ceil")
        tf.Tensor(tf.Operation(desc))
    end

"""
     floor(x)

Returns element-wise largest integer not greater than x.


"""
tf.@op function floor(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Floor")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Floor")
        tf.Tensor(tf.Operation(desc))
    end

"""
     sqrt(x)

Computes square root of x element-wise.

I.e., \\(y = \sqrt{x} = x^{1/2}\\).
"""
tf.@op function sqrt(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Sqrt")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Sqrt")
        tf.Tensor(tf.Operation(desc))
    end

"""
     abs(x)

Computes the absolute value of a tensor.

Given a tensor `x`, this operation returns a tensor containing the absolute
value of each element in `x`. For example, if x is an input element and y is
an output element, this operation computes \\(y = |x|\\).
"""
tf.@op function abs(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Abs")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Abs")
        tf.Tensor(tf.Operation(desc))
    end

"""
     cos(x)

Computes cos of x element-wise.


"""
tf.@op function cos(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Cos")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Cos")
        tf.Tensor(tf.Operation(desc))
    end

"""
     sin(x)

Computes sin of x element-wise.


"""
tf.@op function sin(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Sin")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Sin")
        tf.Tensor(tf.Operation(desc))
    end

"""
     tan(x)

Computes tan of x element-wise.


"""
tf.@op function tan(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Tan")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Tan")
        tf.Tensor(tf.Operation(desc))
    end

"""
     atan(x)

Computes atan of x element-wise.


"""
tf.@op function atan(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Atan")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Atan")
        tf.Tensor(tf.Operation(desc))
    end

"""
     asin(x)

Computes asin of x element-wise.


"""
tf.@op function asin(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Asin")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Asin")
        tf.Tensor(tf.Operation(desc))
    end

"""
     acos(x)

Computes acos of x element-wise.


"""
tf.@op function acos(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Acos")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Acos")
        tf.Tensor(tf.Operation(desc))
    end

"""
     tanh(x)

Computes hyperbolic tangent of `x` element-wise.


"""
tf.@op function tanh(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Tanh")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Tanh")
        tf.Tensor(tf.Operation(desc))
    end

"""
     lgamma(x)

Computes the log of the absolute value of `Gamma(x)` element-wise.


"""
tf.@op function lgamma(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Lgamma")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Lgamma")
        tf.Tensor(tf.Operation(desc))
    end

"""
     erf(x)

Computes the Gauss error function of `x` element-wise.


"""
tf.@op function erf(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Erf")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Erf")
        tf.Tensor(tf.Operation(desc))
    end

"""
     erfc(x)

Computes the complementary error function of `x` element-wise.


"""
tf.@op function erfc(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Erfc")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Erfc")
        tf.Tensor(tf.Operation(desc))
    end

"""
     real(input)

Returns the real part of a complex number.

Given a tensor `input` of complex numbers, this operation returns a tensor of
type `float` that is the real part of each element in `input`. All elements in
`input` must be complex numbers of the form \\(a + bj\\), where *a* is the real
 part returned by this operation and *b* is the imaginary part.

For example:

```
# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.real(input) ==> [-2.25, 3.25]
```
"""
tf.@op function real(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Real")
                    input_ = convert(TensorFlow.Tensor{Complex{Float32}}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "Real")
        tf.Tensor(tf.Operation(desc))
    end

"""
     imag(input)

Returns the imaginary part of a complex number.

Given a tensor `input` of complex numbers, this operation returns a tensor of
type `float` that is the imaginary part of each element in `input`. All
elements in `input` must be complex numbers of the form \\(a + bj\\), where *a*
is the real part and *b* is the imaginary part returned by this operation.

For example:

```
# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.imag(input) ==> [4.75, 5.75]
```
"""
tf.@op function imag(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Imag")
                    input_ = convert(TensorFlow.Tensor{Complex{Float32}}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "Imag")
        tf.Tensor(tf.Operation(desc))
    end

"""
     sign(x)

Returns an element-wise indication of the sign of a number.

`y = sign(x) = -1` if `x < 0`; 0 if `x == 0`; 1 if `x > 0`.

For complex numbers, `y = sign(x) = x / |x|` if `x != 0`, otherwise `y = 0`.
"""
tf.@op function sign(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Sign")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Sign")
        tf.Tensor(tf.Operation(desc))
    end

"""
     conj(input)

Returns the complex conjugate of a complex number.

Given a tensor `input` of complex numbers, this operation returns a tensor of
complex numbers that are the complex conjugate of each element in `input`. The
complex numbers in `input` must be of the form \\(a + bj\\), where *a* is the
real part and *b* is the imaginary part.

The complex conjugate returned by this operation is of the form \\(a - bj\\).

For example:

```
# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.conj(input) ==> [-2.25 - 4.75j, 3.25 - 5.75j]
```
"""
tf.@op function conj(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Conj")
                    input_ = convert(TensorFlow.Tensor{Complex{Float32}}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "Conj")
        tf.Tensor(tf.Operation(desc))
    end

"""
     round(x)

Rounds the values of a tensor to the nearest integer, element-wise.

Rounds half to even.  Also known as bankers rounding. If you want to round
according to the current system rounding mode use std::cint.
"""
tf.@op function round(x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Round")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                end), name, "Round")
        tf.Tensor(tf.Operation(desc))
    end

"""
     polygamma(a, x)

Compute the polygamma function \\(\psi^{(n)}(x)\\).

The polygamma function is defined as:


\\(\psi^{(n)}(x) = \frac{d^n}{dx^n} \psi(x)\\)

where \\(\psi(x)\\) is the digamma function.
"""
tf.@op function polygamma(a_, x_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Polygamma")
                    a_ = convert(TensorFlow.Tensor{Any}, a_)
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (a_, x_) = tf.tf_promote(a_, x_)
                    tf.add_input(desc, a_)
                    tf.add_input(desc, x_)
                end), name, "Polygamma")
        tf.Tensor(tf.Operation(desc))
    end

"""
     zeta(x, q)

Compute the Hurwitz zeta function \\(\zeta(x, q)\\).

The Hurwitz zeta function is defined as:


\\(\zeta(x, q) = \sum_{n=0}^{\infty} (q + n)^{-x}\\)
"""
tf.@op function zeta(x_, q_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Zeta")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    q_ = convert(TensorFlow.Tensor{Any}, q_)
                    (x_, q_) = tf.tf_promote(x_, q_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, q_)
                end), name, "Zeta")
        tf.Tensor(tf.Operation(desc))
    end

"""
     matrix_inverse(input; adjoint=false)

Computes the inverse of one or more square invertible matrices or their

adjoints (conjugate transposes).

The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions
form square matrices. The output is a tensor of the same shape as the input
containing the inverse for all input submatrices `[..., :, :]`.

The op uses LU decomposition with partial pivoting to compute the inverses.

If a matrix is not invertible there is no guarantee what the op does. It
may detect the condition and raise an exception or it may simply return a
garbage result.
"""
tf.@op function matrix_inverse(input_; name=nothing, adjoint=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MatrixInverse")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if adjoint !== nothing
                        desc["adjoint"] = Base.Bool(adjoint)
                    end
                end), name, "MatrixInverse")
        tf.Tensor(tf.Operation(desc))
    end

"""
     matrix_determinant(input)

Computes the determinant of one ore more square matrices.

The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions
form square matrices. The output is a tensor containing the determinants
for all input submatrices `[..., :, :]`.
"""
tf.@op function matrix_determinant(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MatrixDeterminant")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "MatrixDeterminant")
        tf.Tensor(tf.Operation(desc))
    end

"""
     diag(diagonal)

Returns a diagonal tensor with a given diagonal values.

Given a `diagonal`, this operation returns a tensor with the `diagonal` and
everything else padded with zeros. The diagonal is computed as follows:

Assume `diagonal` has dimensions [D1,..., Dk], then the output is a tensor of
rank 2k with dimensions [D1,..., Dk, D1,..., Dk] where:

`output[i1,..., ik, i1,..., ik] = diagonal[i1, ..., ik]` and 0 everywhere else.

For example:

```
# 'diagonal' is [1, 2, 3, 4]
tf.diag(diagonal) ==> [[1, 0, 0, 0]
                       [0, 2, 0, 0]
                       [0, 0, 3, 0]
                       [0, 0, 0, 4]]
```
"""
tf.@op function diag(diagonal_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Diag")
                    diagonal_ = convert(TensorFlow.Tensor{Any}, diagonal_)
                    (diagonal_,) = tf.tf_promote(diagonal_)
                    tf.add_input(desc, diagonal_)
                end), name, "Diag")
        tf.Tensor(tf.Operation(desc))
    end

"""
     matrix_diag_part(input)

Returns the batched diagonal part of a batched tensor.

This operation returns a tensor with the `diagonal` part
of the batched `input`. The `diagonal` part is computed as follows:

Assume `input` has `k` dimensions `[I, J, K, ..., M, N]`, then the output is a
tensor of rank `k - 1` with dimensions `[I, J, K, ..., min(M, N)]` where:

`diagonal[i, j, k, ..., n] = input[i, j, k, ..., n, n]`.

The input must be at least a matrix.

For example:

```
# 'input' is [[[1, 0, 0, 0]
               [0, 2, 0, 0]
               [0, 0, 3, 0]
               [0, 0, 0, 4]],
              [[5, 0, 0, 0]
               [0, 6, 0, 0]
               [0, 0, 7, 0]
               [0, 0, 0, 8]]]

and input.shape = (2, 4, 4)

tf.matrix_diag_part(input) ==> [[1, 2, 3, 4], [5, 6, 7, 8]]

which has shape (2, 4)
```
"""
tf.@op function matrix_diag_part(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("MatrixDiagPart")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "MatrixDiagPart")
        tf.Tensor(tf.Operation(desc))
    end

"""
     cast(x)

Cast x of type SrcT to y of DstT.


"""
tf.@op function cast(x_; name=nothing, SrcT=nothing, DstT=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Cast")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                    if SrcT !== nothing
                        desc["SrcT"] = Base.identity(SrcT)
                    end
                    if DstT !== nothing
                        desc["DstT"] = Base.identity(DstT)
                    end
                end), name, "Cast")
        tf.Tensor(tf.Operation(desc))
    end

"""
     one_hot(indices, depth, on_value, off_value; axis=-1)

Returns a one-hot tensor.

The locations represented by indices in `indices` take value `on_value`,
while all other locations take value `off_value`.

If the input `indices` is rank `N`, the output will have rank `N+1`,
The new axis is created at dimension `axis` (default: the new axis is
appended at the end).

If `indices` is a scalar the output shape will be a vector of length `depth`.

If `indices` is a vector of length `features`, the output shape will be:
```
  features x depth if axis == -1
  depth x features if axis == 0
```

If `indices` is a matrix (batch) with shape `[batch, features]`,
the output shape will be:
```
  batch x features x depth if axis == -1
  batch x depth x features if axis == 1
  depth x batch x features if axis == 0
```


Examples
=========

Suppose that

```
  indices = [0, 2, -1, 1]
  depth = 3
  on_value = 5.0
  off_value = 0.0
  axis = -1
```

Then output is `[4 x 3]`:

    ```output =
      [5.0 0.0 0.0]  // one_hot(0)
      [0.0 0.0 5.0]  // one_hot(2)
      [0.0 0.0 0.0]  // one_hot(-1)
      [0.0 5.0 0.0]  // one_hot(1)
    ```

Suppose that

```
  indices = [0, 2, -1, 1]
  depth = 3
  on_value = 0.0
  off_value = 3.0
  axis = 0
```

Then output is `[3 x 4]`:

    ```output =
      [0.0 3.0 3.0 3.0]
      [3.0 3.0 3.0 0.0]
      [3.0 3.0 3.0 3.0]
      [3.0 0.0 3.0 3.0]
    //  ^                one_hot(0)
    //      ^            one_hot(2)
    //          ^        one_hot(-1)
    //              ^    one_hot(1)
    ```
Suppose that

```
  indices = [[0, 2], [1, -1]]
  depth = 3
  on_value = 1.0
  off_value = 0.0
  axis = -1
```

Then output is `[2 x 2 x 3]`:

    ```output =
      [
        [1.0, 0.0, 0.0]  // one_hot(0)
        [0.0, 0.0, 1.0]  // one_hot(2)
      ][
        [0.0, 1.0, 0.0]  // one_hot(1)
        [0.0, 0.0, 0.0]  // one_hot(-1)
      ]```
"""
tf.@op function one_hot(indices_, depth_, on_value_, off_value_; name=nothing, axis=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("OneHot")
                    indices_ = convert(TensorFlow.Tensor{Int64}, indices_)
                    indices_ = indices_ - convert(tf.Tensor{eltype(indices_)}, 1)
                    depth_ = convert(TensorFlow.Tensor{Int32}, depth_)
                    on_value_ = convert(TensorFlow.Tensor{Any}, on_value_)
                    off_value_ = convert(TensorFlow.Tensor{Any}, off_value_)
                    (on_value_, off_value_) = tf.tf_promote(on_value_, off_value_)
                    (indices_,) = tf.tf_promote(indices_)
                    tf.add_input(desc, indices_)
                    tf.add_input(desc, depth_)
                    tf.add_input(desc, on_value_)
                    tf.add_input(desc, off_value_)
                    if axis !== nothing
                        axis = Base.Int(axis) - 1
                    end
                    if axis !== nothing
                        desc["axis"] = Base.Int(axis)
                    end
                end), name, "OneHot")
        tf.Tensor(tf.Operation(desc))
    end

"""
     reshape(tensor, shape)

Reshapes a tensor.

Given `tensor`, this operation returns a tensor that has the same values
as `tensor` with shape `shape`.

If one component of `shape` is the special value -1, the size of that dimension
is computed so that the total size remains constant.  In particular, a `shape`
of `[-1]` flattens into 1-D.  At most one component of `shape` can be -1.

If `shape` is 1-D or higher, then the operation returns a tensor with shape
`shape` filled with the values of `tensor`. In this case, the number of elements
implied by `shape` must be the same as the number of elements in `tensor`.

For example:

```
# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]
# tensor 't' has shape [9]
reshape(t, [3, 3]) ==> [[1, 2, 3],
                        [4, 5, 6],
                        [7, 8, 9]]

# tensor 't' is [[[1, 1], [2, 2]],
#                [[3, 3], [4, 4]]]
# tensor 't' has shape [2, 2, 2]
reshape(t, [2, 4]) ==> [[1, 1, 2, 2],
                        [3, 3, 4, 4]]

# tensor 't' is [[[1, 1, 1],
#                 [2, 2, 2]],
#                [[3, 3, 3],
#                 [4, 4, 4]],
#                [[5, 5, 5],
#                 [6, 6, 6]]]
# tensor 't' has shape [3, 2, 3]
# pass '[-1]' to flatten 't'
reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]

# -1 can also be used to infer the shape

# -1 is inferred to be 9:
reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 2:
reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 3:
reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],
                              [2, 2, 2],
                              [3, 3, 3]],
                             [[4, 4, 4],
                              [5, 5, 5],
                              [6, 6, 6]]]

# tensor 't' is [7]
# shape `[]` reshapes to a scalar
reshape(t, []) ==> 7
```
"""
tf.@op function reshape(tensor_, shape_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Reshape")
                    tensor_ = convert(TensorFlow.Tensor{Any}, tensor_)
                    shape_ = convert(TensorFlow.Tensor{Int32}, shape_)
                    (tensor_,) = tf.tf_promote(tensor_)
                    (shape_,) = tf.tf_promote(shape_)
                    tf.add_input(desc, tensor_)
                    tf.add_input(desc, shape_)
                end), name, "Reshape")
        tf.Tensor(tf.Operation(desc))
    end

"""
     split(split_dim, value)

Splits a tensor into `num_split` tensors along one dimension.


"""
tf.@op function split(split_dim_, value_; name=nothing, num_split=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Split")
                    split_dim_ = convert(TensorFlow.Tensor{Int32}, split_dim_)
                    split_dim_ = split_dim_ - convert(tf.Tensor{eltype(split_dim_)}, 1)
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (value_,) = tf.tf_promote(value_)
                    tf.add_input(desc, split_dim_)
                    tf.add_input(desc, value_)
                    if num_split !== nothing
                        desc["num_split"] = Base.Int(num_split)
                    end
                end), name, "Split")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:num_split
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     div(x, y)

Returns x / y element-wise.

*NOTE*: `Div` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function div(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Div")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Div")
        tf.Tensor(tf.Operation(desc))
    end

"""
     minimum(x, y)

Returns the min of x and y (i.e. x < y ? x : y) element-wise.

*NOTE*: `Minimum` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function minimum(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Minimum")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Minimum")
        tf.Tensor(tf.Operation(desc))
    end

"""
     maximum(x, y)

Returns the max of x and y (i.e. x > y ? x : y) element-wise.

*NOTE*: `Maximum` supports broadcasting. More about broadcasting
[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
"""
tf.@op function maximum(x_, y_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Maximum")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    y_ = convert(TensorFlow.Tensor{Any}, y_)
                    (x_, y_) = tf.tf_promote(x_, y_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, y_)
                end), name, "Maximum")
        tf.Tensor(tf.Operation(desc))
    end

"""
     select(condition, t, e)

Selects elements from `t` or `e`, depending on `condition`.

The `t`, and `e` tensors must all have the same shape, and the
output will also have that shape.

The `condition` tensor must be a scalar if `t` and `e` are scalars.
If `t` and `e` are vectors or higher rank, then `condition` must be either a
scalar, a vector with size matching the first dimension of `t`, or must have
the same shape as `t`.

The `condition` tensor acts as a mask that chooses, based on the value at each
element, whether the corresponding element / row in the output should be
taken from `t` (if true) or `e` (if false).

If `condition` is a vector and `t` and `e` are higher rank matrices, then
it chooses which row (outer dimension) to copy from `t` and `e`.
If `condition` has the same shape as `t` and `e`, then it chooses which
element to copy from `t` and `e`.

For example:

```prettyprint
# 'condition' tensor is [[True,  False]
#                        [False, True]]
# 't' is [[1, 2],
#         [3, 4]]
# 'e' is [[5, 6],
#         [7, 8]]
select(condition, t, e) ==> [[1, 6],
                             [7, 4]]


# 'condition' tensor is [True, False]
# 't' is [[1, 2],
#         [3, 4]]
# 'e' is [[5, 6],
#         [7, 8]]
select(condition, t, e) ==> [[1, 2],
                             [7, 8]]

```
"""
tf.@op function select(condition_, t_, e_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Select")
                    condition_ = convert(TensorFlow.Tensor{Bool}, condition_)
                    t_ = convert(TensorFlow.Tensor{Any}, t_)
                    e_ = convert(TensorFlow.Tensor{Any}, e_)
                    (t_, e_) = tf.tf_promote(t_, e_)
                    tf.add_input(desc, condition_)
                    tf.add_input(desc, t_)
                    tf.add_input(desc, e_)
                end), name, "Select")
        tf.Tensor(tf.Operation(desc))
    end

"""
     switch(data, pred)

Forwards `data` to the output port determined by `pred`.

If `pred` is true, the `data` input is forwarded to `output_true`. Otherwise,
the data goes to `output_false`.

See also `RefSwitch` and `Merge`.
"""
tf.@op function switch(data_, pred_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Switch")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    pred_ = convert(TensorFlow.Tensor{Bool}, pred_)
                    (data_,) = tf.tf_promote(data_)
                    tf.add_input(desc, data_)
                    tf.add_input(desc, pred_)
                end), name, "Switch")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:2
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     identity(input)

Return a tensor with the same shape and contents as the input tensor or value.


"""
tf.@op function identity(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Identity")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "Identity")
        tf.Tensor(tf.Operation(desc))
    end

"""
     merge(inputs)

Forwards the value of an available tensor from `inputs` to `output`.

`Merge` waits for at least one of the tensors in `inputs` to become available.
It is usually combined with `Switch` to implement branching.

`Merge` forwards the first tensor to become available to `output`, and sets
`value_index` to its index in `inputs`.
"""
tf.@op function merge(inputs_; name=nothing, N=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Merge")
                    inputs_ = [convert(TensorFlow.Tensor{Any}, x) for x = inputs_]
                    (inputs_,) = tf.tf_promote(inputs_)
                    tf.add_input(desc, inputs_)
                    if N !== nothing
                        desc["N"] = Base.Int(N)
                    end
                end), name, "Merge")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:2
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     enter(data; is_constant=false, parallel_iterations=10)

Creates or finds a child frame, and makes `data` available to the child frame.

This op is used together with `Exit` to create loops in the graph.
The unique `frame_name` is used by the `Executor` to identify frames. If
`is_constant` is true, `output` is a constant in the child frame; otherwise
it may be changed in the child frame. At most `parallel_iterations` iterations
are run in parallel in the child frame.
"""
tf.@op function enter(data_; name=nothing, frame_name=nothing, is_constant=nothing, parallel_iterations=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Enter")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    (data_,) = tf.tf_promote(data_)
                    tf.add_input(desc, data_)
                    if frame_name !== nothing
                        desc["frame_name"] = Base.String(frame_name)
                    end
                    if is_constant !== nothing
                        desc["is_constant"] = Base.Bool(is_constant)
                    end
                    if parallel_iterations !== nothing
                        desc["parallel_iterations"] = Base.Int(parallel_iterations)
                    end
                end), name, "Enter")
        tf.Tensor(tf.Operation(desc))
    end

"""
     loop_cond(input)

Forwards the input to the output.

This operator represents the loop termination condition used by the
"pivot" switches of a loop.
"""
tf.@op function loop_cond(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LoopCond")
                    input_ = convert(TensorFlow.Tensor{Bool}, input_)
                    tf.add_input(desc, input_)
                end), name, "LoopCond")
        tf.Tensor(tf.Operation(desc))
    end

"""
     exit(data)

Exits the current frame to its parent frame.

Exit makes its input `data` available to the parent frame.
"""
tf.@op function exit(data_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Exit")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    (data_,) = tf.tf_promote(data_)
                    tf.add_input(desc, data_)
                end), name, "Exit")
        tf.Tensor(tf.Operation(desc))
    end

"""
     next_iteration(data)

Makes its input available to the next iteration.


"""
tf.@op function next_iteration(data_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("NextIteration")
                    data_ = convert(TensorFlow.Tensor{Any}, data_)
                    (data_,) = tf.tf_promote(data_)
                    tf.add_input(desc, data_)
                end), name, "NextIteration")
        tf.Tensor(tf.Operation(desc))
    end

"""
     svd(input; compute_uv=true, full_matrices=false)

Computes the singular value decompositions of one or more matrices.

Computes the SVD of each inner matrix in `input` such that
`input[..., :, :] = u[..., :, :] * diag(s[..., :, :]) * transpose(v[..., :, :])`

```prettyprint
# a is a tensor containing a batch of matrices.
# s is a tensor of singular values for each matrix.
# u is the tensor containing of left singular vectors for each matrix.
# v is the tensor containing of right singular vectors for each matrix.
s, u, v = svd(a)
s, _, _ = svd(a, compute_uv=False)
```
"""
tf.@op function svd(input_; name=nothing, compute_uv=nothing, full_matrices=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Svd")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if compute_uv !== nothing
                        desc["compute_uv"] = Base.Bool(compute_uv)
                    end
                    if full_matrices !== nothing
                        desc["full_matrices"] = Base.Bool(full_matrices)
                    end
                end), name, "Svd")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:3
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     cross(a, b)

Compute the pairwise cross product.

`a` and `b` must be the same shape; they can either be simple 3-element vectors,
or any shape where the innermost dimension is 3. In the latter case, each pair
of corresponding 3-element vectors is cross-multiplied independently.
"""
tf.@op function cross(a_, b_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Cross")
                    a_ = convert(TensorFlow.Tensor{Any}, a_)
                    b_ = convert(TensorFlow.Tensor{Any}, b_)
                    (a_, b_) = tf.tf_promote(a_, b_)
                    tf.add_input(desc, a_)
                    tf.add_input(desc, b_)
                end), name, "Cross")
        tf.Tensor(tf.Operation(desc))
    end

"""
     complex(real, imag)

Converts two real numbers to a complex number.

Given a tensor `real` representing the real part of a complex number, and a
tensor `imag` representing the imaginary part of a complex number, this
operation returns complex numbers elementwise of the form \\(a + bj\\), where
*a* represents the `real` part and *b* represents the `imag` part.

The input tensors `real` and `imag` must have the same shape.

For example:

```
# tensor 'real' is [2.25, 3.25]
# tensor `imag` is [4.75, 5.75]
tf.complex(real, imag) ==> [[2.25 + 4.75j], [3.25 + 5.75j]]
```
"""
tf.@op function complex(real_, imag_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Complex")
                    real_ = convert(TensorFlow.Tensor{Float32}, real_)
                    imag_ = convert(TensorFlow.Tensor{Float32}, imag_)
                    (real_, imag_) = tf.tf_promote(real_, imag_)
                    tf.add_input(desc, real_)
                    tf.add_input(desc, imag_)
                end), name, "Complex")
        tf.Tensor(tf.Operation(desc))
    end

"""
     print(input, data; message=, first_n=-1, summarize=3)

Prints a list of tensors.

Passes `input` through to `output` and prints `data` when evaluating.
"""
tf.@op function print(input_, data_; name=nothing, U=nothing, message=nothing, first_n=nothing, summarize=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Print")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    data_ = [convert(TensorFlow.Tensor{Any}, x) for x = data_]
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, data_)
                    if U !== nothing
                        desc["U"] = map(Base.identity, U)
                    end
                    if message !== nothing
                        desc["message"] = Base.String(message)
                    end
                    if first_n !== nothing
                        desc["first_n"] = Base.Int(first_n)
                    end
                    if summarize !== nothing
                        desc["summarize"] = Base.Int(summarize)
                    end
                end), name, "Print")
        tf.Tensor(tf.Operation(desc))
    end

"""
     reverse_v2(tensor, axis)

Reverses specific dimensions of a tensor.

NOTE `tf.reverse` has now changed behavior in preparation for 1.0.
`tf.reverse_v2` is currently an alias that will be deprecated before TF 1.0.

Given a `tensor`, and a `int32` tensor `axis` representing the set of
dimensions of `tensor` to reverse. This operation reverses each dimension
`i` for which there exists `j` s.t. `axis[j] == i`.

`tensor` can have up to 8 dimensions. The number of dimensions specified
in `axis` may be 0 or more entries. If an index is specified more than
once, a InvalidArgument error is raised.

For example:

```
# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [3] or 'dims' is -1
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
```
"""
tf.@op function reverse_v2(tensor_, axis_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("ReverseV2")
                    tensor_ = convert(TensorFlow.Tensor{Any}, tensor_)
                    axis_ = convert(TensorFlow.Tensor{Int32}, axis_)
                    axis_ = axis_ - convert(tf.Tensor{eltype(axis_)}, 1)
                    (tensor_,) = tf.tf_promote(tensor_)
                    (axis_,) = tf.tf_promote(axis_)
                    tf.add_input(desc, tensor_)
                    tf.add_input(desc, axis_)
                end), name, "ReverseV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     size(input; out_type=Int32)

Returns the size of a tensor.

This operation returns an integer representing the number of elements in
`input`.

For example:

```
# 't' is [[[1, 1,, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]
size(t) ==> 12
```
"""
tf.@op function size(input_; name=nothing, out_type=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Size")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if out_type !== nothing
                        desc["out_type"] = Base.identity(out_type)
                    end
                end), name, "Size")
        tf.Tensor(tf.Operation(desc))
    end

"""
     softmax_cross_entropy_with_logits(features, labels)

Computes softmax cross entropy cost and gradients to backpropagate.

Inputs are the logits, not probabilities.
"""
tf.@op function softmax_cross_entropy_with_logits(features_, labels_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SoftmaxCrossEntropyWithLogits")
                    features_ = convert(TensorFlow.Tensor{Any}, features_)
                    labels_ = convert(TensorFlow.Tensor{Any}, labels_)
                    (features_, labels_) = tf.tf_promote(features_, labels_)
                    tf.add_input(desc, features_)
                    tf.add_input(desc, labels_)
                end), name, "SoftmaxCrossEntropyWithLogits")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:2
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     sparse_softmax_cross_entropy_with_logits(features, labels)

Computes softmax cross entropy cost and gradients to backpropagate.

Unlike `SoftmaxCrossEntropyWithLogits`, this operation does not accept
a matrix of label probabilities, but rather a single label per row
of features.  This label is considered to have probability 1.0 for the
given row.

Inputs are the logits, not probabilities.
"""
tf.@op function sparse_softmax_cross_entropy_with_logits(features_, labels_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("SparseSoftmaxCrossEntropyWithLogits")
                    features_ = convert(TensorFlow.Tensor{Any}, features_)
                    labels_ = convert(TensorFlow.Tensor{Int64}, labels_)
                    (features_,) = tf.tf_promote(features_)
                    (labels_,) = tf.tf_promote(labels_)
                    tf.add_input(desc, features_)
                    tf.add_input(desc, labels_)
                end), name, "SparseSoftmaxCrossEntropyWithLogits")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:2
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     top_kv2(input, k; sorted=true)

Finds values and indices of the `k` largest elements for the last dimension.

If the input is a vector (rank-1), finds the `k` largest entries in the vector
and outputs their values and indices as vectors.  Thus `values[j]` is the
`j`-th largest entry in `input`, and its index is `indices[j]`.

For matrices (resp. higher rank input), computes the top `k` entries in each
row (resp. vector along the last dimension).  Thus,

    values.shape = indices.shape = input.shape[:-1] + [k]

If two elements are equal, the lower-index element appears first.
"""
tf.@op function top_kv2(input_, k_; name=nothing, sorted=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("TopKV2")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    k_ = convert(TensorFlow.Tensor{Int32}, k_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, k_)
                    if sorted !== nothing
                        desc["sorted"] = Base.Bool(sorted)
                    end
                end), name, "TopKV2")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:2
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     in_top_k(predictions, targets)

Says whether the targets are in the top `K` predictions.

This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the
prediction for the target class is among the top `k` predictions among
all predictions for example `i`. Note that the behavior of `InTopK` differs
from the `TopK` op in its handling of ties; if multiple classes have the
same prediction value and straddle the top-`k` boundary, all of those
classes are considered to be in the top `k`.

More formally, let

  \\(predictions_i\\) be the predictions for all classes for example `i`,
  \\(targets_i\\) be the target class for example `i`,
  \\(out_i\\) be the output for example `i`,

out_i = predictions_{i, targets_i} \in TopKIncludingTies(predictions_i)
"""
tf.@op function in_top_k(predictions_, targets_; name=nothing, k=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("InTopK")
                    predictions_ = convert(TensorFlow.Tensor{Float32}, predictions_)
                    targets_ = convert(TensorFlow.Tensor{Int32}, targets_)
                    (targets_,) = tf.tf_promote(targets_)
                    tf.add_input(desc, predictions_)
                    tf.add_input(desc, targets_)
                    if k !== nothing
                        desc["k"] = Base.Int(k)
                    end
                end), name, "InTopK")
        tf.Tensor(tf.Operation(desc))
    end

"""
     fifo_queue_v2(; shapes=Int64[], capacity=-1, container=, shared_name=)

A queue that produces elements in first-in first-out order.


"""
tf.@op function fifo_queue_v2(; name=nothing, component_types=nothing, shapes=nothing, capacity=nothing, container=nothing, shared_name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("FIFOQueueV2")
                    if component_types !== nothing
                        desc["component_types"] = map(Base.identity, component_types)
                    end
                    if shapes !== nothing
                        desc["shapes"] = map(Base.identity, shapes)
                    end
                    if capacity !== nothing
                        desc["capacity"] = Base.Int(capacity)
                    end
                    if container !== nothing
                        desc["container"] = Base.String(container)
                    end
                    if shared_name !== nothing
                        desc["shared_name"] = Base.String(shared_name)
                    end
                end), name, "FIFOQueueV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     random_shuffle_queue_v2(; shapes=Int64[], capacity=-1, min_after_dequeue=0, seed=0, seed2=0, container=, shared_name=)

A queue that randomizes the order of elements.


"""
tf.@op function random_shuffle_queue_v2(; name=nothing, component_types=nothing, shapes=nothing, capacity=nothing, min_after_dequeue=nothing, seed=nothing, seed2=nothing, container=nothing, shared_name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("RandomShuffleQueueV2")
                    if component_types !== nothing
                        desc["component_types"] = map(Base.identity, component_types)
                    end
                    if shapes !== nothing
                        desc["shapes"] = map(Base.identity, shapes)
                    end
                    if capacity !== nothing
                        desc["capacity"] = Base.Int(capacity)
                    end
                    if min_after_dequeue !== nothing
                        desc["min_after_dequeue"] = Base.Int(min_after_dequeue)
                    end
                    if seed !== nothing
                        desc["seed"] = Base.Int(seed)
                    end
                    if seed2 !== nothing
                        desc["seed2"] = Base.Int(seed2)
                    end
                    if container !== nothing
                        desc["container"] = Base.String(container)
                    end
                    if shared_name !== nothing
                        desc["shared_name"] = Base.String(shared_name)
                    end
                end), name, "RandomShuffleQueueV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     queue_enqueue_v2(handle, components; timeout_ms=-1)

Enqueues a tuple of one or more tensors in the given queue.

The components input has k elements, which correspond to the components of
tuples stored in the given queue.

N.B. If the queue is full, this operation will block until the given
element has been enqueued (or 'timeout_ms' elapses, if specified).
"""
tf.@op function queue_enqueue_v2(handle_, components_; name=nothing, Tcomponents=nothing, timeout_ms=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("QueueEnqueueV2")
                    handle_ = convert(TensorFlow.Tensor{Any}, handle_)
                    components_ = [convert(TensorFlow.Tensor{Any}, x) for x = components_]
                    tf.add_input(desc, handle_)
                    tf.add_input(desc, components_)
                    if Tcomponents !== nothing
                        desc["Tcomponents"] = map(Base.identity, Tcomponents)
                    end
                    if timeout_ms !== nothing
                        desc["timeout_ms"] = Base.Int(timeout_ms)
                    end
                end), name, "QueueEnqueueV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     queue_enqueue_many_v2(handle, components; timeout_ms=-1)

Enqueues zero or more tuples of one or more tensors in the given queue.

This operation slices each component tensor along the 0th dimension to
make multiple queue elements. All of the tuple components must have the
same size in the 0th dimension.

The components input has k elements, which correspond to the components of
tuples stored in the given queue.

N.B. If the queue is full, this operation will block until the given
elements have been enqueued (or 'timeout_ms' elapses, if specified).
"""
tf.@op function queue_enqueue_many_v2(handle_, components_; name=nothing, Tcomponents=nothing, timeout_ms=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("QueueEnqueueManyV2")
                    handle_ = convert(TensorFlow.Tensor{Any}, handle_)
                    components_ = [convert(TensorFlow.Tensor{Any}, x) for x = components_]
                    tf.add_input(desc, handle_)
                    tf.add_input(desc, components_)
                    if Tcomponents !== nothing
                        desc["Tcomponents"] = map(Base.identity, Tcomponents)
                    end
                    if timeout_ms !== nothing
                        desc["timeout_ms"] = Base.Int(timeout_ms)
                    end
                end), name, "QueueEnqueueManyV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     queue_size_v2(handle)

Computes the number of elements in the given queue.


"""
tf.@op function queue_size_v2(handle_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("QueueSizeV2")
                    handle_ = convert(TensorFlow.Tensor{Any}, handle_)
                    tf.add_input(desc, handle_)
                end), name, "QueueSizeV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     queue_close_v2(handle; cancel_pending_enqueues=false)

Closes the given queue.

This operation signals that no more elements will be enqueued in the
given queue. Subsequent Enqueue(Many) operations will fail.
Subsequent Dequeue(Many) operations will continue to succeed if
sufficient elements remain in the queue. Subsequent Dequeue(Many)
operations that would block will fail immediately.
"""
tf.@op function queue_close_v2(handle_; name=nothing, cancel_pending_enqueues=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("QueueCloseV2")
                    handle_ = convert(TensorFlow.Tensor{Any}, handle_)
                    tf.add_input(desc, handle_)
                    if cancel_pending_enqueues !== nothing
                        desc["cancel_pending_enqueues"] = Base.Bool(cancel_pending_enqueues)
                    end
                end), name, "QueueCloseV2")
        tf.Tensor(tf.Operation(desc))
    end

"""
     lin_space(start, stop, num)

Generates values in an interval.

A sequence of `num` evenly-spaced values are generated beginning at `start`.
If `num > 1`, the values in the sequence increase by `stop - start / num - 1`,
so that the last one is exactly `stop`.

For example:

```
tf.linspace(10.0, 12.0, 3, name="linspace") => [ 10.0  11.0  12.0]
```
"""
tf.@op function lin_space(start_, stop_, num_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("LinSpace")
                    start_ = convert(TensorFlow.Tensor{Any}, start_)
                    stop_ = convert(TensorFlow.Tensor{Any}, stop_)
                    num_ = convert(TensorFlow.Tensor{Int32}, num_)
                    num_ = num_ - convert(tf.Tensor{eltype(num_)}, 1)
                    (start_, stop_) = tf.tf_promote(start_, stop_)
                    (num_,) = tf.tf_promote(num_)
                    tf.add_input(desc, start_)
                    tf.add_input(desc, stop_)
                    tf.add_input(desc, num_)
                end), name, "LinSpace")
        tf.Tensor(tf.Operation(desc))
    end

"""
     range(start, limit, delta)

Creates a sequence of numbers.

This operation creates a sequence of numbers that begins at `start` and
extends by increments of `delta` up to but not including `limit`.

For example:

```
# 'start' is 3
# 'limit' is 18
# 'delta' is 3
tf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]
```
"""
tf.@op function range(start_, limit_, delta_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Range")
                    start_ = convert(TensorFlow.Tensor{Int32}, start_)
                    limit_ = convert(TensorFlow.Tensor{Int32}, limit_)
                    delta_ = convert(TensorFlow.Tensor{Int32}, delta_)
                    (start_, limit_, delta_) = tf.tf_promote(start_, limit_, delta_)
                    tf.add_input(desc, start_)
                    tf.add_input(desc, limit_)
                    tf.add_input(desc, delta_)
                end), name, "Range")
        tf.Tensor(tf.Operation(desc))
    end

"""
     fill(dims, value)

Creates a tensor filled with a scalar value.

This operation creates a tensor of shape `dims` and fills it with `value`.

For example:

```
# Output tensor has shape [2, 3].
fill([2, 3], 9) ==> [[9, 9, 9]
                     [9, 9, 9]]
```
"""
tf.@op function fill(dims_, value_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Fill")
                    dims_ = convert(TensorFlow.Tensor{Int32}, dims_)
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (value_,) = tf.tf_promote(value_)
                    tf.add_input(desc, dims_)
                    tf.add_input(desc, value_)
                end), name, "Fill")
        tf.Tensor(tf.Operation(desc))
    end

"""
     squeeze(input; squeeze_dims=Int64[])

Removes dimensions of size 1 from the shape of a tensor.

Given a tensor `input`, this operation returns a tensor of the same type with
all dimensions of size 1 removed. If you don't want to remove all size 1
dimensions, you can remove specific size 1 dimensions by specifying
`squeeze_dims`.

For example:

```
# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
shape(squeeze(t)) ==> [2, 3]
```

Or, to remove specific size 1 dimensions:

```
# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
shape(squeeze(t, [2, 4])) ==> [1, 2, 3, 1]
```
"""
tf.@op function squeeze(input_; name=nothing, squeeze_dims=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Squeeze")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                    if squeeze_dims !== nothing
                        desc["squeeze_dims"] = map(Base.identity, squeeze_dims)
                    end
                end), name, "Squeeze")
        tf.Tensor(tf.Operation(desc))
    end

"""
     unpack(value; axis=0)

Unpacks a given dimension of a rank-`R` tensor into `num` rank-`(R-1)` tensors.

Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.
For example, given a tensor of shape `(A, B, C, D)`;

If `axis == 0` then the i'th tensor in `output` is the slice `value[i, :, :, :]`
  and each tensor in `output` will have shape `(B, C, D)`. (Note that the
  dimension unpacked along is gone, unlike `split`).

If `axis == 1` then the i'th tensor in `output` is the slice `value[:, i, :, :]`
  and each tensor in `output` will have shape `(A, C, D)`.
Etc.

This is the opposite of `pack`.
"""
tf.@op function unpack(value_; name=nothing, num=nothing, axis=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Unpack")
                    value_ = convert(TensorFlow.Tensor{Any}, value_)
                    (value_,) = tf.tf_promote(value_)
                    tf.add_input(desc, value_)
                    if num !== nothing
                        desc["num"] = Base.Int(num)
                    end
                    if axis !== nothing
                        axis = Base.Int(axis) - 1
                    end
                    if axis !== nothing
                        desc["axis"] = Base.Int(axis)
                    end
                end), name, "Unpack")
        out = tf.Tensor[]
        op = tf.Operation(desc)
        for out_idx = 1:num
            push!(out, tf.Tensor(op, out_idx))
        end
        out
    end

"""
     transpose(x, perm)

Shuffle dimensions of x according to a permutation.

The output `y` has the same rank as `x`. The shapes of `x` and `y` satisfy:
  `y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]`
"""
tf.@op function transpose(x_, perm_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Transpose")
                    x_ = convert(TensorFlow.Tensor{Any}, x_)
                    perm_ = convert(TensorFlow.Tensor{Int32}, perm_)
                    (perm_,) = tf.tf_promote(perm_)
                    (x_,) = tf.tf_promote(x_)
                    tf.add_input(desc, x_)
                    tf.add_input(desc, perm_)
                end), name, "Transpose")
        tf.Tensor(tf.Operation(desc))
    end

"""
     slice(input, begin, size)

Return a slice from 'input'.

The output tensor is a tensor with dimensions described by 'size'
whose values are extracted from 'input' starting at the offsets in
'begin'.

*Requirements*:
  0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n)
"""
tf.@op function slice(input_, begin_, size_; name=nothing, Index=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Slice")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    begin_ = convert(TensorFlow.Tensor{Any}, begin_)
                    begin_ = begin_ - convert(tf.Tensor{eltype(begin_)}, 1)
                    size_ = convert(TensorFlow.Tensor{Any}, size_)
                    (input_,) = tf.tf_promote(input_)
                    (begin_, size_) = tf.tf_promote(begin_, size_)
                    tf.add_input(desc, input_)
                    tf.add_input(desc, begin_)
                    tf.add_input(desc, size_)
                    if Index !== nothing
                        desc["Index"] = Base.identity(Index)
                    end
                end), name, "Slice")
        tf.Tensor(tf.Operation(desc))
    end

"""
     rank(input)

Returns the rank of a tensor.

This operation returns an integer representing the rank of `input`.

For example:

```
# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]
# shape of tensor 't' is [2, 2, 3]
rank(t) ==> 3
```

**Note**: The rank of a tensor is not the same as the rank of a matrix. The rank
of a tensor is the number of indices required to uniquely select each element
of the tensor. Rank is also known as "order", "degree", or "ndims."
"""
tf.@op function rank(input_; name=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Rank")
                    input_ = convert(TensorFlow.Tensor{Any}, input_)
                    (input_,) = tf.tf_promote(input_)
                    tf.add_input(desc, input_)
                end), name, "Rank")
        tf.Tensor(tf.Operation(desc))
    end

"""
     conv2d_backprop_input(input_sizes, filter, out_backprop; use_cudnn_on_gpu=true, data_format=NHWC)

Computes the gradients of convolution with respect to the input.


"""
tf.@op function conv2d_backprop_input(input_sizes_, filter_, out_backprop_; name=nothing, strides=nothing, use_cudnn_on_gpu=nothing, padding=nothing, data_format=nothing)
        local desc
        tf.with_op_name((()->begin 
                    desc = tf.NodeDescription("Conv2DBackpropInput")
                    input_sizes_ = convert(TensorFlow.Tensor{Int32}, input_sizes_)
                    filter_ = convert(TensorFlow.Tensor{Any}, filter_)
                    out_backprop_ = convert(TensorFlow.Tensor{Any}, out_backprop_)
                    (filter_, out_backprop_) = tf.tf_promote(filter_, out_backprop_)
                    tf.add_input(desc, input_sizes_)
                    tf.add_input(desc, filter_)
                    tf.add_input(desc, out_backprop_)
                    if strides !== nothing
                        desc["strides"] = map(Base.identity, strides)
                    end
                    if use_cudnn_on_gpu !== nothing
                        desc["use_cudnn_on_gpu"] = Base.Bool(use_cudnn_on_gpu)
                    end
                    if padding !== nothing
                        desc["padding"] = Base.String(padding)
                    end
                    if data_format !== nothing
                        desc["data_format"] = Base.String(data_format)
                    end
                end), name, "Conv2DBackpropInput")
        tf.Tensor(tf.Operation(desc))
    end

end
